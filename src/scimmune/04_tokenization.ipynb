{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bff8265",
   "metadata": {},
   "source": [
    "# Immune Data Tokenization\n",
    "\n",
    "## Convert to HF dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e9d0bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import ScImmuneTokenizer # refactored version\n",
    "\n",
    "import scanpy as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f68e46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s5srinivasan/immune-foundational-model/.venv/lib64/python3.9/site-packages/anndata/_core/anndata.py:1754: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"obs\")\n"
     ]
    }
   ],
   "source": [
    "## Set data folder\n",
    "data_path = \"../data/cellxgene_data\"\n",
    "\n",
    "## Load dataset\n",
    "adata_immune = sc.read_h5ad(f\"{data_path}/immune_1M_merged.h5ad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c65523bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_immune.obs[\"disease_ontology_term_id\"] = \"DOID:4\" # to disease ontology root ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2524fad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61048"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = ScImmuneTokenizer(vocab_file=\"vocab_with_metadata.json\") # initialize tokenizer\n",
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3f22b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [1:11:40<00:00, 232.55it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the fields from obs to be turned into metadata tokens\n",
    "\n",
    "metadata_fields = [\n",
    "                    \"cell_type_ontology_term_id\",\n",
    "                    \"self_reported_ethnicity_ontology_term_id\", \n",
    "                    \"tissue_general_ontology_term_id\",\n",
    "                    \"development_stage_ontology_term_id\",\n",
    "                    \"sex_ontology_term_id\",\n",
    "                    \"disease_ontology_term_id\"\n",
    "]\n",
    "\n",
    "gene_names = adata_immune.var.feature_name.values # get gene names\n",
    "\n",
    "tokenized_input_ids = []\n",
    "tokenized_values = []\n",
    "\n",
    "for i in tqdm(range(adata_immune.n_obs)):\n",
    "    \n",
    "    # 1. Get dense expression vector\n",
    "    row = adata_immune.X[i]\n",
    "    if not isinstance(row, np.ndarray):\n",
    "        row = row.toarray().squeeze()\n",
    "\n",
    "    # 2. Get metadata tokens\n",
    "    obs_row = adata_immune.obs.iloc[i]\n",
    "    metadata_tokens = []\n",
    "    for field in metadata_fields:\n",
    "        val = obs_row.get(field)\n",
    "        if isinstance(val, str) and val != \"NA\" and \"=\" not in val:\n",
    "            token = f\"<{field.split('_ontology_term_id')[0]}={val}>\"\n",
    "            metadata_tokens.append(token)\n",
    "\n",
    "    # 3. Tokenize\n",
    "    tokenized = tokenizer.tokenize_cell_batch(\n",
    "        data=np.expand_dims(row, axis=0),\n",
    "        gene_ids=gene_names,\n",
    "        metadata_tokens=metadata_tokens,\n",
    "        append_cls=True,\n",
    "        include_zero_gene=False\n",
    "    )\n",
    "    \n",
    "    input_ids, values = tokenized[0]\n",
    "    tokenized_input_ids.append(input_ids)\n",
    "    tokenized_values.append(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b33d0349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Padding and truncation happen in DataCollator, not here\n",
    "hf_dataset = Dataset.from_dict({\n",
    "    \"genes\": tokenized_input_ids,\n",
    "    \"values\": tokenized_values\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32aacc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset.set_format(type=\"torch\", columns=[\"genes\", \"values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ea98f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (44/44 shards): 100%|██████████| 1000000/1000000 [00:07<00:00, 136979.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "hf_dataset.save_to_disk(\"scimmune-model/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
