{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8a4fbd7",
   "metadata": {},
   "source": [
    "# Fine-tuning on Pre-trained Model for Cell-type Annotation\n",
    "In this tutorial, we demonstrate how to fine-tune a pre-trained model on a new dataset for the cell type annotation task. We use the Multiple Sclerosis dataset as an example and fine-tune on the pre-trained whole-body model. Please download the dataset folder from https://drive.google.com/drive/folders/1Qd42YNabzyr2pWt9xoY4cVMTAxsNBt4v?usp=sharing\n",
    "\n",
    "We summarize the fine-tuning pipeline in the following steps, which can be used as a general recipe for finetuning on cell-type annotation tasks and beyond: \n",
    "\n",
    "     1. Specify hyper-parameter setup for integration task\n",
    "     \n",
    "     2. Load and pre-process data\n",
    "     \n",
    "     3. Load the pre-trained scGPT model\n",
    "     \n",
    "     4. Finetune scGPT with task-specific objectives\n",
    "     \n",
    "     5. Evaluate fine-tuned scGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9406b4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s5srinivasan/py39env/lib64/python3.9/site-packages/scgpt/model/model.py:21: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n",
      "/home/s5srinivasan/py39env/lib64/python3.9/site-packages/scgpt/model/multiomic_model.py:19: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n",
      "/home/s5srinivasan/py39env/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n",
      "/home/s5srinivasan/py39env/lib64/python3.9/site-packages/scanpy/_settings.py:488: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  IPython.display.set_matplotlib_formats(*ipython_format)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from typing import List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "import pandas as pd\n",
    "# from . import asyn\n",
    "import pickle\n",
    "import torch\n",
    "from anndata import AnnData\n",
    "import scanpy as sc\n",
    "# import scvi\n",
    "import wandb\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.sparse import issparse\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "import scgpt as scg\n",
    "from scgpt.model import TransformerModel, AdversarialDiscriminator\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch, random_mask_value\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    masked_relative_error,\n",
    "    criterion_neg_log_bernoulli,\n",
    ")\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.preprocess import Preprocessor\n",
    "from scgpt import SubsetsBatchSampler\n",
    "from scgpt.utils import set_seed, category_str2int, eval_scib_metrics\n",
    "\n",
    "sc.set_figure_params(figsize=(6, 6))\n",
    "os.environ[\"KMP_WARNINGS\"] = \"off\"\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, inject_adapter_in_model\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3b5d67",
   "metadata": {},
   "source": [
    "## Step1: Specify hyper-parameter setup for cell-type annotation task\n",
    "Listed below are some hyper-parameter recommendations for the cell-type task. Note that the CLS objective is on to facilitate cell-type classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d07b5257",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_defaults = dict(\n",
    "    seed=0,\n",
    "    dataset_name=\"covid\",\n",
    "    do_train=True,\n",
    "    load_model=\"/home/s5srinivasan/covid-annotation-scgpt/save/scgpt-human\",\n",
    "    mask_ratio=0.0,\n",
    "    epochs=10,\n",
    "    n_bins=51,\n",
    "    MVC=False, # Masked value prediction for cell embedding\n",
    "    ecs_thres=0.0, # Elastic cell similarity objective, 0.0 to 1.0, 0.0 to disable\n",
    "    dab_weight=0.0,\n",
    "    lr=1e-4,\n",
    "    batch_size=8,\n",
    "    layer_size=128,\n",
    "    nlayers=4,  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "    nhead=4,  # number of heads in nn.MultiheadAttention\n",
    "    dropout=0.2,  # dropout probability\n",
    "    schedule_ratio=0.9,  # ratio of epochs for learning rate schedule\n",
    "    save_eval_interval=5,\n",
    "    fast_transformer=True,\n",
    "    pre_norm=False,\n",
    "    amp=True,  # Automatic Mixed Precision\n",
    "    include_zero_gene = False,\n",
    "    freeze = False, #freeze\n",
    "    DSBN = False,  # Domain-spec batchnorm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "256da779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msrks\u001b[0m (\u001b[33msrks-uc-san-diego\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/s5srinivasan/immune-foundational-model/src/code/wandb/run-20250506_180422-y9yc17un</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/srks-uc-san-diego/scGPT-covid-annotation/runs/y9yc17un' target=\"_blank\">flowing-microwave-193</a></strong> to <a href='https://wandb.ai/srks-uc-san-diego/scGPT-covid-annotation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/srks-uc-san-diego/scGPT-covid-annotation' target=\"_blank\">https://wandb.ai/srks-uc-san-diego/scGPT-covid-annotation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/srks-uc-san-diego/scGPT-covid-annotation/runs/y9yc17un' target=\"_blank\">https://wandb.ai/srks-uc-san-diego/scGPT-covid-annotation/runs/y9yc17un</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seed': 0, 'dataset_name': 'covid', 'do_train': True, 'load_model': '/home/s5srinivasan/covid-annotation-scgpt/save/scgpt-human', 'mask_ratio': 0.0, 'epochs': 10, 'n_bins': 51, 'MVC': False, 'ecs_thres': 0.0, 'dab_weight': 0.0, 'lr': 0.0001, 'batch_size': 8, 'layer_size': 128, 'nlayers': 4, 'nhead': 4, 'dropout': 0.2, 'schedule_ratio': 0.9, 'save_eval_interval': 5, 'fast_transformer': True, 'pre_norm': False, 'amp': True, 'include_zero_gene': False, 'freeze': False, 'DSBN': False}\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    config=hyperparameter_defaults,\n",
    "    project=\"scGPT-covid-annotation\",\n",
    "    reinit=True,\n",
    "    settings=wandb.Settings(start_method=\"fork\"),\n",
    ")\n",
    "config = wandb.config\n",
    "print(config)\n",
    "\n",
    "set_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d7890b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for input and preprocessing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "mask_ratio = config.mask_ratio\n",
    "mask_value = \"auto\"  # for masked values, now it should always be auto\n",
    "\n",
    "include_zero_gene = config.include_zero_gene  # if True, include zero genes among hvgs in the training\n",
    "max_seq_len = 3001\n",
    "n_bins = config.n_bins\n",
    "\n",
    "# input/output representation\n",
    "input_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
    "output_style = \"binned\"  # \"normed_raw\", \"log1p\", or \"binned\"\n",
    "\n",
    "# settings for training\n",
    "MLM = False  # whether to use masked language modeling, currently it is always on.\n",
    "CLS = True  # celltype classification objective\n",
    "ADV = False  # Adversarial training for batch correction\n",
    "CCE = False  # Contrastive cell embedding objective\n",
    "MVC = config.MVC  # Masked value prediction for cell embedding\n",
    "ECS = config.ecs_thres > 0  # Elastic cell similarity objective\n",
    "DAB = False  # Domain adaptation by reverse backpropagation, set to 2 for separate optimizer\n",
    "INPUT_BATCH_LABELS = False  # TODO: have these help MLM and MVC, while not to classifier\n",
    "input_emb_style = \"continuous\"  # \"category\" or \"continuous\" or \"scaling\"\n",
    "cell_emb_style = \"cls\"  # \"avg-pool\" or \"w-pool\" or \"cls\"\n",
    "adv_E_delay_epochs = 0  # delay adversarial training on encoder for a few epochs\n",
    "adv_D_delay_epochs = 0\n",
    "mvc_decoder_style = \"inner product\"\n",
    "ecs_threshold = config.ecs_thres\n",
    "dab_weight = config.dab_weight\n",
    "\n",
    "explicit_zero_prob = MLM and include_zero_gene  # whether explicit bernoulli for zeros\n",
    "do_sample_in_train = False and explicit_zero_prob  # sample the bernoulli in training\n",
    "\n",
    "per_seq_batch_sample = False\n",
    "\n",
    "# settings for optimizer\n",
    "lr = config.lr  # TODO: test learning rate ratio between two tasks\n",
    "lr_ADV = 1e-3  # learning rate for discriminator, used when ADV is True\n",
    "batch_size = config.batch_size\n",
    "eval_batch_size = config.batch_size\n",
    "epochs = config.epochs\n",
    "schedule_interval = 1\n",
    "\n",
    "# settings for the model\n",
    "fast_transformer = config.fast_transformer\n",
    "fast_transformer_backend = \"flash\"  # \"linear\" or \"flash\"\n",
    "embsize = config.layer_size  # embedding dimension\n",
    "d_hid = config.layer_size  # dimension of the feedforward network in TransformerEncoder\n",
    "nlayers = config.nlayers  # number of TransformerEncoderLayer in TransformerEncoder\n",
    "nhead = config.nhead  # number of heads in nn.MultiheadAttention\n",
    "dropout = config.dropout  # dropout probability\n",
    "\n",
    "# logging\n",
    "log_interval = 100  # iterations\n",
    "save_eval_interval = config.save_eval_interval  # epochs\n",
    "do_eval_scib_metrics = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17ff2309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% validate settings\n",
    "assert input_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "assert output_style in [\"normed_raw\", \"log1p\", \"binned\"]\n",
    "assert input_emb_style in [\"category\", \"continuous\", \"scaling\"]\n",
    "if input_style == \"binned\":\n",
    "    if input_emb_style == \"scaling\":\n",
    "        raise ValueError(\"input_emb_style `scaling` is not supported for binned input.\")\n",
    "elif input_style == \"log1p\" or input_style == \"normed_raw\":\n",
    "    if input_emb_style == \"category\":\n",
    "        raise ValueError(\n",
    "            \"input_emb_style `category` is not supported for log1p or normed_raw input.\"\n",
    "        )\n",
    "\n",
    "if input_emb_style == \"category\":\n",
    "    mask_value = n_bins + 1\n",
    "    pad_value = n_bins  # for padding gene expr values\n",
    "    n_input_bins = n_bins + 2\n",
    "else:\n",
    "    mask_value = -1\n",
    "    pad_value = -2\n",
    "    n_input_bins = n_bins\n",
    "\n",
    "if ADV and DAB:\n",
    "    raise ValueError(\"ADV and DAB cannot be both True.\")\n",
    "DAB_separate_optim = True if DAB > 1 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf7112d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to save/dev_covid-May06-18-04\n"
     ]
    }
   ],
   "source": [
    "dataset_name = config.dataset_name\n",
    "save_dir = Path(f\"./save/dev_{dataset_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"save to {save_dir}\")\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, save_dir / \"run.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e897e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sc.read_h5ad(\"../data/datasets/czi_covid_pbmc_2pct.h5ad\")\n",
    "test.obs[\"batch\"] = 0\n",
    "test.var\n",
    "del test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fc7002",
   "metadata": {},
   "source": [
    "## Step 2: Load and pre-process data\n",
    "We follow the standard scGPT data pre-processing pipelines for the cell-type annotation task. Note that since now we have two datasets at hand (i.e., reference and query data), the same pre-prpocessing steps need to be applied to both of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95b50200",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == \"covid\":\n",
    "    data_dir = Path(\"../data/datasets\")\n",
    "    adata = sc.read(data_dir / \"czi_covid_pbmc_5pct.h5ad\")\n",
    "    adata_test = sc.read(data_dir / \"czi_covid_pbmc_2pct.h5ad\")\n",
    "    adata.obs[\"celltype\"] = adata.obs[\"cell_type\"].astype(\"category\")\n",
    "    adata_test.obs[\"celltype\"] = adata_test.obs[\"cell_type\"].astype(\"category\")\n",
    "    adata.obs[\"batch_id\"]  = adata.obs[\"batch\"] = \"0\"\n",
    "    adata_test.obs[\"batch_id\"]  = adata_test.obs[\"batch\"] = \"1\"          \n",
    "    adata.var.set_index(adata.var[\"feature_name\"], inplace=True)\n",
    "    adata_test.var.set_index(adata.var[\"feature_name\"], inplace=True)\n",
    "    data_is_raw = False\n",
    "    filter_gene_by_counts = False\n",
    "    adata_test_raw = adata_test.copy()\n",
    "    adata = adata.concatenate(adata_test, batch_key=\"batch\")\n",
    "                \n",
    "# make the batch category column\n",
    "batch_id_labels = adata.obs[\"batch\"].astype(\"category\").cat.codes.values\n",
    "adata.obs[\"batch_id\"] = batch_id_labels\n",
    "celltype_id_labels = adata.obs[\"celltype\"].astype(\"category\").cat.codes.values\n",
    "celltypes = adata.obs[\"celltype\"].unique()\n",
    "num_types = len(np.unique(celltype_id_labels))\n",
    "id2type = dict(enumerate(adata.obs[\"celltype\"].astype(\"category\").cat.categories))\n",
    "adata.obs[\"celltype_id\"] = celltype_id_labels\n",
    "adata.var[\"feature_name\"] = adata.var.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed94f596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/datasets\n"
     ]
    }
   ],
   "source": [
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea338e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>assay</th>\n",
       "      <th>assay_ontology_term_id</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>cell_type_ontology_term_id</th>\n",
       "      <th>development_stage</th>\n",
       "      <th>development_stage_ontology_term_id</th>\n",
       "      <th>disease</th>\n",
       "      <th>disease_ontology_term_id</th>\n",
       "      <th>donor_id</th>\n",
       "      <th>...</th>\n",
       "      <th>tissue_general_ontology_term_id</th>\n",
       "      <th>raw_sum</th>\n",
       "      <th>nnz</th>\n",
       "      <th>raw_mean_nnz</th>\n",
       "      <th>raw_variance_nnz</th>\n",
       "      <th>n_measured_vars</th>\n",
       "      <th>batch</th>\n",
       "      <th>celltype</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>celltype_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soma_joinid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64892599-0</th>\n",
       "      <td>c7775e88-49bf-4ba2-a03b-93f00447c958</td>\n",
       "      <td>10x 3' transcription profiling</td>\n",
       "      <td>EFO:0030003</td>\n",
       "      <td>central memory CD4-positive, alpha-beta T cell</td>\n",
       "      <td>CL:0000904</td>\n",
       "      <td>seventh decade stage</td>\n",
       "      <td>HsapDv:0000241</td>\n",
       "      <td>COVID-19</td>\n",
       "      <td>MONDO:0100096</td>\n",
       "      <td>C-8905</td>\n",
       "      <td>...</td>\n",
       "      <td>UBERON:0000178</td>\n",
       "      <td>1309.0</td>\n",
       "      <td>679</td>\n",
       "      <td>1.927835</td>\n",
       "      <td>7.040507</td>\n",
       "      <td>24677</td>\n",
       "      <td>0</td>\n",
       "      <td>central memory CD4-positive, alpha-beta T cell</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64757274-0</th>\n",
       "      <td>c7775e88-49bf-4ba2-a03b-93f00447c958</td>\n",
       "      <td>10x 3' transcription profiling</td>\n",
       "      <td>EFO:0030003</td>\n",
       "      <td>CD14-positive monocyte</td>\n",
       "      <td>CL:0001054</td>\n",
       "      <td>eighth decade stage</td>\n",
       "      <td>HsapDv:0000242</td>\n",
       "      <td>COVID-19</td>\n",
       "      <td>MONDO:0100096</td>\n",
       "      <td>C-8884</td>\n",
       "      <td>...</td>\n",
       "      <td>UBERON:0000178</td>\n",
       "      <td>5597.0</td>\n",
       "      <td>1786</td>\n",
       "      <td>3.133819</td>\n",
       "      <td>160.498049</td>\n",
       "      <td>24677</td>\n",
       "      <td>0</td>\n",
       "      <td>CD14-positive monocyte</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82379513-0</th>\n",
       "      <td>9dbab10c-118d-496b-966a-67f1763a6b7d</td>\n",
       "      <td>10x 5' v2</td>\n",
       "      <td>EFO:0009900</td>\n",
       "      <td>B cell</td>\n",
       "      <td>CL:0000236</td>\n",
       "      <td>46-year-old stage</td>\n",
       "      <td>HsapDv:0000140</td>\n",
       "      <td>COVID-19</td>\n",
       "      <td>MONDO:0100096</td>\n",
       "      <td>P-M040</td>\n",
       "      <td>...</td>\n",
       "      <td>UBERON:0000178</td>\n",
       "      <td>3008.0</td>\n",
       "      <td>1123</td>\n",
       "      <td>2.678540</td>\n",
       "      <td>36.904593</td>\n",
       "      <td>27647</td>\n",
       "      <td>0</td>\n",
       "      <td>B cell</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81503076-0</th>\n",
       "      <td>9dbab10c-118d-496b-966a-67f1763a6b7d</td>\n",
       "      <td>10x 5' v2</td>\n",
       "      <td>EFO:0009900</td>\n",
       "      <td>mature gamma-delta T cell</td>\n",
       "      <td>CL:0000800</td>\n",
       "      <td>26-year-old stage</td>\n",
       "      <td>HsapDv:0000120</td>\n",
       "      <td>COVID-19</td>\n",
       "      <td>MONDO:0100096</td>\n",
       "      <td>P-M061</td>\n",
       "      <td>...</td>\n",
       "      <td>UBERON:0000178</td>\n",
       "      <td>3213.0</td>\n",
       "      <td>1457</td>\n",
       "      <td>2.205216</td>\n",
       "      <td>28.671456</td>\n",
       "      <td>27647</td>\n",
       "      <td>0</td>\n",
       "      <td>mature gamma-delta T cell</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81665438-0</th>\n",
       "      <td>9dbab10c-118d-496b-966a-67f1763a6b7d</td>\n",
       "      <td>10x 5' v2</td>\n",
       "      <td>EFO:0009900</td>\n",
       "      <td>CD14-positive, CD16-negative classical monocyte</td>\n",
       "      <td>CL:0002057</td>\n",
       "      <td>65-year-old stage</td>\n",
       "      <td>HsapDv:0000159</td>\n",
       "      <td>COVID-19</td>\n",
       "      <td>MONDO:0100096</td>\n",
       "      <td>P-S088</td>\n",
       "      <td>...</td>\n",
       "      <td>UBERON:0000178</td>\n",
       "      <td>11439.0</td>\n",
       "      <td>2681</td>\n",
       "      <td>4.266692</td>\n",
       "      <td>344.230715</td>\n",
       "      <td>27647</td>\n",
       "      <td>0</td>\n",
       "      <td>CD14-positive, CD16-negative classical monocyte</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65067247-1</th>\n",
       "      <td>c7775e88-49bf-4ba2-a03b-93f00447c958</td>\n",
       "      <td>10x 3' transcription profiling</td>\n",
       "      <td>EFO:0030003</td>\n",
       "      <td>platelet</td>\n",
       "      <td>CL:0000233</td>\n",
       "      <td>76-year-old stage</td>\n",
       "      <td>HsapDv:0000170</td>\n",
       "      <td>COVID-19</td>\n",
       "      <td>MONDO:0100096</td>\n",
       "      <td>AP9</td>\n",
       "      <td>...</td>\n",
       "      <td>UBERON:0000178</td>\n",
       "      <td>781.0</td>\n",
       "      <td>413</td>\n",
       "      <td>1.891041</td>\n",
       "      <td>13.830332</td>\n",
       "      <td>24677</td>\n",
       "      <td>1</td>\n",
       "      <td>platelet</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65124177-1</th>\n",
       "      <td>c7775e88-49bf-4ba2-a03b-93f00447c958</td>\n",
       "      <td>10x 3' transcription profiling</td>\n",
       "      <td>EFO:0030003</td>\n",
       "      <td>IgG plasma cell</td>\n",
       "      <td>CL:0000985</td>\n",
       "      <td>51-year-old stage</td>\n",
       "      <td>HsapDv:0000145</td>\n",
       "      <td>COVID-19</td>\n",
       "      <td>MONDO:0100096</td>\n",
       "      <td>CV0200</td>\n",
       "      <td>...</td>\n",
       "      <td>UBERON:0000178</td>\n",
       "      <td>3038.0</td>\n",
       "      <td>1075</td>\n",
       "      <td>2.826047</td>\n",
       "      <td>35.888706</td>\n",
       "      <td>24677</td>\n",
       "      <td>1</td>\n",
       "      <td>IgG plasma cell</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58062959-1</th>\n",
       "      <td>ebc2e1ff-c8f9-466a-acf4-9d291afaf8b3</td>\n",
       "      <td>10x 5' v1</td>\n",
       "      <td>EFO:0011025</td>\n",
       "      <td>classical monocyte</td>\n",
       "      <td>CL:0000860</td>\n",
       "      <td>ninth decade stage</td>\n",
       "      <td>HsapDv:0000243</td>\n",
       "      <td>COVID-19</td>\n",
       "      <td>MONDO:0100096</td>\n",
       "      <td>N00040</td>\n",
       "      <td>...</td>\n",
       "      <td>UBERON:0000178</td>\n",
       "      <td>4278.0</td>\n",
       "      <td>1342</td>\n",
       "      <td>3.187779</td>\n",
       "      <td>346.043758</td>\n",
       "      <td>37298</td>\n",
       "      <td>1</td>\n",
       "      <td>classical monocyte</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58384414-1</th>\n",
       "      <td>ebc2e1ff-c8f9-466a-acf4-9d291afaf8b3</td>\n",
       "      <td>10x 5' v1</td>\n",
       "      <td>EFO:0011025</td>\n",
       "      <td>dendritic cell</td>\n",
       "      <td>CL:0000451</td>\n",
       "      <td>sixth decade stage</td>\n",
       "      <td>HsapDv:0000240</td>\n",
       "      <td>COVID-19</td>\n",
       "      <td>MONDO:0100096</td>\n",
       "      <td>S00065</td>\n",
       "      <td>...</td>\n",
       "      <td>UBERON:0000178</td>\n",
       "      <td>7858.0</td>\n",
       "      <td>2074</td>\n",
       "      <td>3.788814</td>\n",
       "      <td>114.786059</td>\n",
       "      <td>37298</td>\n",
       "      <td>1</td>\n",
       "      <td>dendritic cell</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82295765-1</th>\n",
       "      <td>9dbab10c-118d-496b-966a-67f1763a6b7d</td>\n",
       "      <td>10x 5' v2</td>\n",
       "      <td>EFO:0009900</td>\n",
       "      <td>CD14-positive, CD16-negative classical monocyte</td>\n",
       "      <td>CL:0002057</td>\n",
       "      <td>51-year-old stage</td>\n",
       "      <td>HsapDv:0000145</td>\n",
       "      <td>COVID-19</td>\n",
       "      <td>MONDO:0100096</td>\n",
       "      <td>P-M036</td>\n",
       "      <td>...</td>\n",
       "      <td>UBERON:0000178</td>\n",
       "      <td>3855.0</td>\n",
       "      <td>1522</td>\n",
       "      <td>2.532852</td>\n",
       "      <td>29.974265</td>\n",
       "      <td>27647</td>\n",
       "      <td>1</td>\n",
       "      <td>CD14-positive, CD16-negative classical monocyte</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>286308 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       dataset_id  \\\n",
       "soma_joinid                                         \n",
       "64892599-0   c7775e88-49bf-4ba2-a03b-93f00447c958   \n",
       "64757274-0   c7775e88-49bf-4ba2-a03b-93f00447c958   \n",
       "82379513-0   9dbab10c-118d-496b-966a-67f1763a6b7d   \n",
       "81503076-0   9dbab10c-118d-496b-966a-67f1763a6b7d   \n",
       "81665438-0   9dbab10c-118d-496b-966a-67f1763a6b7d   \n",
       "...                                           ...   \n",
       "65067247-1   c7775e88-49bf-4ba2-a03b-93f00447c958   \n",
       "65124177-1   c7775e88-49bf-4ba2-a03b-93f00447c958   \n",
       "58062959-1   ebc2e1ff-c8f9-466a-acf4-9d291afaf8b3   \n",
       "58384414-1   ebc2e1ff-c8f9-466a-acf4-9d291afaf8b3   \n",
       "82295765-1   9dbab10c-118d-496b-966a-67f1763a6b7d   \n",
       "\n",
       "                                      assay assay_ontology_term_id  \\\n",
       "soma_joinid                                                          \n",
       "64892599-0   10x 3' transcription profiling            EFO:0030003   \n",
       "64757274-0   10x 3' transcription profiling            EFO:0030003   \n",
       "82379513-0                        10x 5' v2            EFO:0009900   \n",
       "81503076-0                        10x 5' v2            EFO:0009900   \n",
       "81665438-0                        10x 5' v2            EFO:0009900   \n",
       "...                                     ...                    ...   \n",
       "65067247-1   10x 3' transcription profiling            EFO:0030003   \n",
       "65124177-1   10x 3' transcription profiling            EFO:0030003   \n",
       "58062959-1                        10x 5' v1            EFO:0011025   \n",
       "58384414-1                        10x 5' v1            EFO:0011025   \n",
       "82295765-1                        10x 5' v2            EFO:0009900   \n",
       "\n",
       "                                                   cell_type  \\\n",
       "soma_joinid                                                    \n",
       "64892599-0    central memory CD4-positive, alpha-beta T cell   \n",
       "64757274-0                            CD14-positive monocyte   \n",
       "82379513-0                                            B cell   \n",
       "81503076-0                         mature gamma-delta T cell   \n",
       "81665438-0   CD14-positive, CD16-negative classical monocyte   \n",
       "...                                                      ...   \n",
       "65067247-1                                          platelet   \n",
       "65124177-1                                   IgG plasma cell   \n",
       "58062959-1                                classical monocyte   \n",
       "58384414-1                                    dendritic cell   \n",
       "82295765-1   CD14-positive, CD16-negative classical monocyte   \n",
       "\n",
       "            cell_type_ontology_term_id     development_stage  \\\n",
       "soma_joinid                                                    \n",
       "64892599-0                  CL:0000904  seventh decade stage   \n",
       "64757274-0                  CL:0001054   eighth decade stage   \n",
       "82379513-0                  CL:0000236     46-year-old stage   \n",
       "81503076-0                  CL:0000800     26-year-old stage   \n",
       "81665438-0                  CL:0002057     65-year-old stage   \n",
       "...                                ...                   ...   \n",
       "65067247-1                  CL:0000233     76-year-old stage   \n",
       "65124177-1                  CL:0000985     51-year-old stage   \n",
       "58062959-1                  CL:0000860    ninth decade stage   \n",
       "58384414-1                  CL:0000451    sixth decade stage   \n",
       "82295765-1                  CL:0002057     51-year-old stage   \n",
       "\n",
       "            development_stage_ontology_term_id   disease  \\\n",
       "soma_joinid                                                \n",
       "64892599-0                      HsapDv:0000241  COVID-19   \n",
       "64757274-0                      HsapDv:0000242  COVID-19   \n",
       "82379513-0                      HsapDv:0000140  COVID-19   \n",
       "81503076-0                      HsapDv:0000120  COVID-19   \n",
       "81665438-0                      HsapDv:0000159  COVID-19   \n",
       "...                                        ...       ...   \n",
       "65067247-1                      HsapDv:0000170  COVID-19   \n",
       "65124177-1                      HsapDv:0000145  COVID-19   \n",
       "58062959-1                      HsapDv:0000243  COVID-19   \n",
       "58384414-1                      HsapDv:0000240  COVID-19   \n",
       "82295765-1                      HsapDv:0000145  COVID-19   \n",
       "\n",
       "            disease_ontology_term_id donor_id  ...  \\\n",
       "soma_joinid                                    ...   \n",
       "64892599-0             MONDO:0100096   C-8905  ...   \n",
       "64757274-0             MONDO:0100096   C-8884  ...   \n",
       "82379513-0             MONDO:0100096   P-M040  ...   \n",
       "81503076-0             MONDO:0100096   P-M061  ...   \n",
       "81665438-0             MONDO:0100096   P-S088  ...   \n",
       "...                              ...      ...  ...   \n",
       "65067247-1             MONDO:0100096      AP9  ...   \n",
       "65124177-1             MONDO:0100096   CV0200  ...   \n",
       "58062959-1             MONDO:0100096   N00040  ...   \n",
       "58384414-1             MONDO:0100096   S00065  ...   \n",
       "82295765-1             MONDO:0100096   P-M036  ...   \n",
       "\n",
       "             tissue_general_ontology_term_id  raw_sum   nnz raw_mean_nnz  \\\n",
       "soma_joinid                                                                \n",
       "64892599-0                    UBERON:0000178   1309.0   679     1.927835   \n",
       "64757274-0                    UBERON:0000178   5597.0  1786     3.133819   \n",
       "82379513-0                    UBERON:0000178   3008.0  1123     2.678540   \n",
       "81503076-0                    UBERON:0000178   3213.0  1457     2.205216   \n",
       "81665438-0                    UBERON:0000178  11439.0  2681     4.266692   \n",
       "...                                      ...      ...   ...          ...   \n",
       "65067247-1                    UBERON:0000178    781.0   413     1.891041   \n",
       "65124177-1                    UBERON:0000178   3038.0  1075     2.826047   \n",
       "58062959-1                    UBERON:0000178   4278.0  1342     3.187779   \n",
       "58384414-1                    UBERON:0000178   7858.0  2074     3.788814   \n",
       "82295765-1                    UBERON:0000178   3855.0  1522     2.532852   \n",
       "\n",
       "            raw_variance_nnz n_measured_vars batch  \\\n",
       "soma_joinid                                          \n",
       "64892599-0          7.040507           24677     0   \n",
       "64757274-0        160.498049           24677     0   \n",
       "82379513-0         36.904593           27647     0   \n",
       "81503076-0         28.671456           27647     0   \n",
       "81665438-0        344.230715           27647     0   \n",
       "...                      ...             ...   ...   \n",
       "65067247-1         13.830332           24677     1   \n",
       "65124177-1         35.888706           24677     1   \n",
       "58062959-1        346.043758           37298     1   \n",
       "58384414-1        114.786059           37298     1   \n",
       "82295765-1         29.974265           27647     1   \n",
       "\n",
       "                                                    celltype batch_id  \\\n",
       "soma_joinid                                                             \n",
       "64892599-0    central memory CD4-positive, alpha-beta T cell        0   \n",
       "64757274-0                            CD14-positive monocyte        0   \n",
       "82379513-0                                            B cell        0   \n",
       "81503076-0                         mature gamma-delta T cell        0   \n",
       "81665438-0   CD14-positive, CD16-negative classical monocyte        0   \n",
       "...                                                      ...      ...   \n",
       "65067247-1                                          platelet        1   \n",
       "65124177-1                                   IgG plasma cell        1   \n",
       "58062959-1                                classical monocyte        1   \n",
       "58384414-1                                    dendritic cell        1   \n",
       "82295765-1   CD14-positive, CD16-negative classical monocyte        1   \n",
       "\n",
       "            celltype_id  \n",
       "soma_joinid              \n",
       "64892599-0           36  \n",
       "64757274-0            3  \n",
       "82379513-0            0  \n",
       "81503076-0           69  \n",
       "81665438-0            4  \n",
       "...                 ...  \n",
       "65067247-1           90  \n",
       "65124177-1           21  \n",
       "58062959-1           39  \n",
       "58384414-1           41  \n",
       "82295765-1            4  \n",
       "\n",
       "[286308 rows x 31 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ec176a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['central memory CD4-positive, alpha-beta T cell', 'CD14-positive monocyte', 'B cell', 'mature gamma-delta T cell', 'CD14-positive, CD16-negative classical monocyte', ..., 'B-2 B cell', 'T-helper 2 cell', 'innate lymphoid cell', 'myeloid lineage restricted progenitor cell', 'T-helper 17 cell']\n",
       "Length: 97\n",
       "Categories (97, object): ['B cell', 'B-2 B cell', 'CD14-low, CD16-positive monocyte', 'CD14-positive monocyte', ..., 'stem cell', 'transitional stage B cell', 'unknown', 'unswitched memory B cell']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obs.celltype.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dc5a6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - match 40568/61891 genes in vocabulary of size 60697.\n",
      "scGPT - INFO - Resume model from /home/s5srinivasan/covid-annotation-scgpt/save/scgpt-human/best_model.pt, the model args will override the config /home/s5srinivasan/covid-annotation-scgpt/save/scgpt-human/args.json.\n"
     ]
    }
   ],
   "source": [
    "if config.load_model is not None:\n",
    "    model_dir = Path(config.load_model)\n",
    "    model_config_file = model_dir / \"args.json\"\n",
    "    model_file = model_dir / \"best_model.pt\"\n",
    "    vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    shutil.copy(vocab_file, save_dir / \"vocab.json\")\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "\n",
    "    adata.var[\"id_in_vocab\"] = [\n",
    "        1 if gene in vocab else -1 for gene in adata.var[\"feature_name\"]\n",
    "    ]\n",
    "    gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "    logger.info(\n",
    "        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(vocab)}.\"\n",
    "    )\n",
    "    adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n",
    "\n",
    "    # model\n",
    "    with open(model_config_file, \"r\") as f:\n",
    "        model_configs = json.load(f)\n",
    "    logger.info(\n",
    "        f\"Resume model from {model_file}, the model args will override the \"\n",
    "        f\"config {model_config_file}.\"\n",
    "    )\n",
    "    embsize = model_configs[\"embsize\"]\n",
    "    nhead = model_configs[\"nheads\"]\n",
    "    d_hid = model_configs[\"d_hid\"]\n",
    "    nlayers = model_configs[\"nlayers\"]\n",
    "    n_layers_cls = model_configs[\"n_layers_cls\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d08757ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Binning data ...\n"
     ]
    }
   ],
   "source": [
    "# set up the preprocessor, use the args to config the workflow\n",
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=filter_gene_by_counts,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=False,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X_binned\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "\n",
    "\n",
    "adata_test = adata[adata.obs[\"batch\"] == \"1\"]\n",
    "adata = adata[adata.obs[\"batch\"] == \"0\"]\n",
    "\n",
    "preprocessor(adata, batch_key=None)\n",
    "preprocessor(adata_test, batch_key=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbc1b20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_key = {  # the values of this map coorespond to the keys in preprocessing\n",
    "    \"normed_raw\": \"X_normed\",\n",
    "    \"log1p\": \"X_normed\",\n",
    "    \"binned\": \"X_binned\",\n",
    "}[input_style]\n",
    "all_counts = (\n",
    "    adata.layers[input_layer_key].toarray()\n",
    "    if issparse(adata.layers[input_layer_key])\n",
    "    else adata.layers[input_layer_key]\n",
    ")\n",
    "genes = adata.var[\"feature_name\"].tolist()\n",
    "\n",
    "celltypes_labels = adata.obs[\"celltype_id\"].tolist()  # make sure count from 0\n",
    "celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "num_batch_types = len(set(batch_ids))\n",
    "batch_ids = np.array(batch_ids)\n",
    "\n",
    "(\n",
    "    train_data,\n",
    "    valid_data,\n",
    "    train_celltype_labels,\n",
    "    valid_celltype_labels,\n",
    "    train_batch_labels,\n",
    "    valid_batch_labels,\n",
    ") = train_test_split(\n",
    "    all_counts, celltypes_labels, batch_ids, test_size=0.1, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cd701b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.load_model is None:\n",
    "    vocab = Vocab(\n",
    "        VocabPybind(genes + special_tokens, None)\n",
    "    )  # bidirectional lookup [gene <-> int]\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(vocab(genes), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "818bfcc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - train set number of samples: 184055, \n",
      "\t feature length: 3001\n",
      "scGPT - INFO - valid set number of samples: 20451, \n",
      "\t feature length: 3001\n"
     ]
    }
   ],
   "source": [
    "tokenized_train = tokenize_and_pad_batch(\n",
    "    train_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,  # append <cls> token at the beginning\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "tokenized_valid = tokenize_and_pad_batch(\n",
    "    valid_data,\n",
    "    gene_ids,\n",
    "    max_len=max_seq_len,\n",
    "    vocab=vocab,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    append_cls=True,\n",
    "    include_zero_gene=include_zero_gene,\n",
    ")\n",
    "logger.info(\n",
    "    f\"train set number of samples: {tokenized_train['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_train['genes'].shape[1]}\"\n",
    ")\n",
    "logger.info(\n",
    "    f\"valid set number of samples: {tokenized_valid['genes'].shape[0]}, \"\n",
    "    f\"\\n\\t feature length: {tokenized_valid['genes'].shape[1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1437ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(profile=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37a80818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(sort_seq_batch=False) -> Tuple[Dict[str, torch.Tensor]]:\n",
    "    masked_values_train = random_mask_value(\n",
    "        tokenized_train[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "    masked_values_valid = random_mask_value(\n",
    "        tokenized_valid[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "    print(\n",
    "        f\"random masking at epoch {epoch:3d}, ratio of masked values in train: \",\n",
    "        f\"{(masked_values_train == mask_value).sum() / (masked_values_train - pad_value).count_nonzero():.4f}\",\n",
    "    )\n",
    "\n",
    "    input_gene_ids_train, input_gene_ids_valid = (\n",
    "        tokenized_train[\"genes\"],\n",
    "        tokenized_valid[\"genes\"],\n",
    "    )\n",
    "    input_values_train, input_values_valid = masked_values_train, masked_values_valid\n",
    "    target_values_train, target_values_valid = (\n",
    "        tokenized_train[\"values\"],\n",
    "        tokenized_valid[\"values\"],\n",
    "    )\n",
    "\n",
    "    tensor_batch_labels_train = torch.from_numpy(train_batch_labels).long()\n",
    "    tensor_batch_labels_valid = torch.from_numpy(valid_batch_labels).long()\n",
    "\n",
    "    tensor_celltype_labels_train = torch.from_numpy(train_celltype_labels).long()\n",
    "    tensor_celltype_labels_valid = torch.from_numpy(valid_celltype_labels).long()\n",
    "\n",
    "    if sort_seq_batch:  # TODO: update to random pick seq source in each traning batch\n",
    "        train_sort_ids = np.argsort(train_batch_labels)\n",
    "        input_gene_ids_train = input_gene_ids_train[train_sort_ids]\n",
    "        input_values_train = input_values_train[train_sort_ids]\n",
    "        target_values_train = target_values_train[train_sort_ids]\n",
    "        tensor_batch_labels_train = tensor_batch_labels_train[train_sort_ids]\n",
    "        tensor_celltype_labels_train = tensor_celltype_labels_train[train_sort_ids]\n",
    "\n",
    "        valid_sort_ids = np.argsort(valid_batch_labels)\n",
    "        input_gene_ids_valid = input_gene_ids_valid[valid_sort_ids]\n",
    "        input_values_valid = input_values_valid[valid_sort_ids]\n",
    "        target_values_valid = target_values_valid[valid_sort_ids]\n",
    "        tensor_batch_labels_valid = tensor_batch_labels_valid[valid_sort_ids]\n",
    "        tensor_celltype_labels_valid = tensor_celltype_labels_valid[valid_sort_ids]\n",
    "\n",
    "    train_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_train,\n",
    "        \"values\": input_values_train,\n",
    "        \"target_values\": target_values_train,\n",
    "        \"batch_labels\": tensor_batch_labels_train,\n",
    "        \"celltype_labels\": tensor_celltype_labels_train,\n",
    "    }\n",
    "    valid_data_pt = {\n",
    "        \"gene_ids\": input_gene_ids_valid,\n",
    "        \"values\": input_values_valid,\n",
    "        \"target_values\": target_values_valid,\n",
    "        \"batch_labels\": tensor_batch_labels_valid,\n",
    "        \"celltype_labels\": tensor_celltype_labels_valid,\n",
    "    }\n",
    "\n",
    "    return train_data_pt, valid_data_pt\n",
    "\n",
    "\n",
    "# dataset\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, data: Dict[str, torch.Tensor]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "\n",
    "\n",
    "# data_loader\n",
    "def prepare_dataloader(\n",
    "    data_pt: Dict[str, torch.Tensor],\n",
    "    batch_size: int,\n",
    "    shuffle: bool = False,\n",
    "    intra_domain_shuffle: bool = False,\n",
    "    drop_last: bool = False,\n",
    "    num_workers: int = 0,\n",
    ") -> DataLoader:\n",
    "    if num_workers == 0:\n",
    "        num_workers = min(len(os.sched_getaffinity(0)), batch_size // 2)\n",
    "\n",
    "    dataset = SeqDataset(data_pt)\n",
    "\n",
    "    if per_seq_batch_sample:\n",
    "        # find the indices of samples in each seq batch\n",
    "        subsets = []\n",
    "        batch_labels_array = data_pt[\"batch_labels\"].numpy()\n",
    "        for batch_label in np.unique(batch_labels_array):\n",
    "            batch_indices = np.where(batch_labels_array == batch_label)[0].tolist()\n",
    "            subsets.append(batch_indices)\n",
    "        data_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_sampler=SubsetsBatchSampler(\n",
    "                subsets,\n",
    "                batch_size,\n",
    "                intra_subset_shuffle=intra_domain_shuffle,\n",
    "                inter_subset_shuffle=shuffle,\n",
    "                drop_last=drop_last,\n",
    "            ),\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "        return data_loader\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return data_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77105fda",
   "metadata": {},
   "source": [
    "## Step 3: Load the pre-trained scGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219bb9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "model = TransformerModel(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    nlayers_cls=3,\n",
    "    n_cls=num_types if CLS else 1,\n",
    "    vocab=vocab,\n",
    "    dropout=dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    do_mvc=MVC,\n",
    "    do_dab=DAB,\n",
    "    use_batch_labels=INPUT_BATCH_LABELS,\n",
    "    num_batch_labels=num_batch_types,\n",
    "    domain_spec_batchnorm=config.DSBN,\n",
    "    input_emb_style=input_emb_style,\n",
    "    n_input_bins=n_input_bins,\n",
    "    cell_emb_style=cell_emb_style,\n",
    "    mvc_decoder_style=mvc_decoder_style,\n",
    "    ecs_threshold=ecs_threshold,\n",
    "    explicit_zero_prob=explicit_zero_prob,\n",
    "    use_fast_transformer=fast_transformer,\n",
    "    fast_transformer_backend=fast_transformer_backend,\n",
    "    pre_norm=config.pre_norm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b6040a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, module in model.named_modules():\n",
    "#     if isinstance(module, torch.nn.Linear):\n",
    "#         print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1962e7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.0.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.0.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.weight with shape torch.Size([1, 512])\n",
      "scGPT - INFO - Loading params decoder.fc.4.bias with shape torch.Size([1])\n",
      "--------------------\n",
      "name: encoder.embedding.weight\n",
      "--------------------\n",
      "name: encoder.enc_norm.weight\n",
      "--------------------\n",
      "name: encoder.enc_norm.bias\n",
      "--------------------\n",
      "name: value_encoder.linear1.weight\n",
      "--------------------\n",
      "name: value_encoder.linear1.bias\n",
      "--------------------\n",
      "name: value_encoder.linear2.weight\n",
      "--------------------\n",
      "name: value_encoder.linear2.bias\n",
      "--------------------\n",
      "name: value_encoder.norm.weight\n",
      "--------------------\n",
      "name: value_encoder.norm.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.self_attn.base_layer.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.self_attn.base_layer.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.self_attn.base_layer.out_proj.base_layer.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.self_attn.base_layer.out_proj.base_layer.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.self_attn.base_layer.out_proj.lora_A.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.self_attn.base_layer.out_proj.lora_B.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.self_attn.lora_A.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.self_attn.lora_B.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.0.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.self_attn.base_layer.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.self_attn.base_layer.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.self_attn.base_layer.out_proj.base_layer.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.self_attn.base_layer.out_proj.base_layer.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.self_attn.base_layer.out_proj.lora_A.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.self_attn.base_layer.out_proj.lora_B.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.self_attn.lora_A.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.self_attn.lora_B.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.1.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.self_attn.base_layer.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.self_attn.base_layer.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.self_attn.base_layer.out_proj.base_layer.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.self_attn.base_layer.out_proj.base_layer.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.self_attn.base_layer.out_proj.lora_A.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.self_attn.base_layer.out_proj.lora_B.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.self_attn.lora_A.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.self_attn.lora_B.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.2.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.self_attn.base_layer.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.self_attn.base_layer.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.self_attn.base_layer.out_proj.base_layer.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.self_attn.base_layer.out_proj.base_layer.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.self_attn.base_layer.out_proj.lora_A.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.self_attn.base_layer.out_proj.lora_B.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.self_attn.lora_A.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.self_attn.lora_B.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.3.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.self_attn.base_layer.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.self_attn.base_layer.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.self_attn.base_layer.out_proj.base_layer.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.self_attn.base_layer.out_proj.base_layer.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.self_attn.base_layer.out_proj.lora_A.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.self_attn.base_layer.out_proj.lora_B.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.self_attn.lora_A.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.self_attn.lora_B.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.4.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.self_attn.base_layer.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.self_attn.base_layer.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.self_attn.base_layer.out_proj.base_layer.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.self_attn.base_layer.out_proj.base_layer.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.self_attn.base_layer.out_proj.lora_A.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.self_attn.base_layer.out_proj.lora_B.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.self_attn.lora_A.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.self_attn.lora_B.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.5.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.self_attn.base_layer.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.self_attn.base_layer.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.self_attn.base_layer.out_proj.base_layer.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.self_attn.base_layer.out_proj.base_layer.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.self_attn.base_layer.out_proj.lora_A.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.self_attn.base_layer.out_proj.lora_B.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.self_attn.lora_A.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.self_attn.lora_B.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.6.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.self_attn.base_layer.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.self_attn.base_layer.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.self_attn.base_layer.out_proj.base_layer.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.self_attn.base_layer.out_proj.base_layer.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.self_attn.base_layer.out_proj.lora_A.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.self_attn.base_layer.out_proj.lora_B.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.self_attn.lora_A.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.self_attn.lora_B.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.7.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.self_attn.base_layer.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.self_attn.base_layer.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.self_attn.base_layer.out_proj.base_layer.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.self_attn.base_layer.out_proj.base_layer.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.self_attn.base_layer.out_proj.lora_A.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.self_attn.base_layer.out_proj.lora_B.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.self_attn.lora_A.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.self_attn.lora_B.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.8.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.self_attn.base_layer.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.self_attn.base_layer.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.self_attn.base_layer.out_proj.base_layer.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.self_attn.base_layer.out_proj.base_layer.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.self_attn.base_layer.out_proj.lora_A.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.self_attn.base_layer.out_proj.lora_B.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.self_attn.lora_A.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.self_attn.lora_B.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.9.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.self_attn.base_layer.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.self_attn.base_layer.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.self_attn.base_layer.out_proj.base_layer.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.self_attn.base_layer.out_proj.base_layer.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.self_attn.base_layer.out_proj.lora_A.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.self_attn.base_layer.out_proj.lora_B.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.self_attn.lora_A.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.self_attn.lora_B.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.10.norm2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.self_attn.base_layer.in_proj_bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.self_attn.base_layer.in_proj_weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.self_attn.base_layer.out_proj.base_layer.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.self_attn.base_layer.out_proj.base_layer.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.self_attn.base_layer.out_proj.lora_A.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.self_attn.base_layer.out_proj.lora_B.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.self_attn.lora_A.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.self_attn.lora_B.default.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.linear1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.linear1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.linear2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.linear2.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.norm1.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.norm1.bias\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.norm2.weight\n",
      "--------------------\n",
      "name: transformer_encoder.layers.11.norm2.bias\n",
      "--------------------\n",
      "name: decoder.fc.0.weight\n",
      "--------------------\n",
      "name: decoder.fc.0.bias\n",
      "--------------------\n",
      "name: decoder.fc.2.weight\n",
      "--------------------\n",
      "name: decoder.fc.2.bias\n",
      "--------------------\n",
      "name: decoder.fc.4.weight\n",
      "--------------------\n",
      "name: decoder.fc.4.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.0.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.0.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.2.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.2.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.3.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.3.bias\n",
      "--------------------\n",
      "name: cls_decoder._decoder.5.weight\n",
      "--------------------\n",
      "name: cls_decoder._decoder.5.bias\n",
      "--------------------\n",
      "name: cls_decoder.out_layer.weight\n",
      "--------------------\n",
      "name: cls_decoder.out_layer.bias\n",
      "scGPT - INFO - Total Pre freeze Params 349282\n",
      "scGPT - INFO - Total Post freeze Params 349282\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"all\",\n",
    "    target_modules=[\"self_attn\"],\n",
    "    task_type=\"SEQ_CLS\"\n",
    ")\n",
    "\n",
    "model = inject_adapter_in_model(lora_config, model)\n",
    "model.parameters()  # Verify trainable parameters\n",
    "\n",
    "if config.load_model is not None:\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "        logger.info(f\"Loading all model params from {model_file}\")\n",
    "    except:\n",
    "        # only load params that are in the model and match the size\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = torch.load(model_file)\n",
    "        pretrained_dict = {\n",
    "            k: v\n",
    "            for k, v in pretrained_dict.items()\n",
    "            if k in model_dict and v.shape == model_dict[k].shape\n",
    "        }\n",
    "        for k, v in pretrained_dict.items():\n",
    "            logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "\n",
    "pre_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "\n",
    "# Freeze all pre-decoder weights\n",
    "for name, para in model.named_parameters():\n",
    "    print(\"-\"*20)\n",
    "    print(f\"name: {name}\")\n",
    "    if config.freeze and \"encoder\" in name and \"transformer_encoder\" not in name:\n",
    "    # if config.freeze and \"encoder\" in name:\n",
    "        print(f\"freezing weights for: {name}\")\n",
    "        para.requires_grad = False\n",
    "\n",
    "post_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "\n",
    "logger.info(f\"Total Pre freeze Params {(pre_freeze_param_count )}\")\n",
    "logger.info(f\"Total Post freeze Params {(post_freeze_param_count )}\")\n",
    "wandb.log(\n",
    "        {\n",
    "            \"info/pre_freeze_param_count\": pre_freeze_param_count,\n",
    "            \"info/post_freeze_param_count\": post_freeze_param_count,\n",
    "        },\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     num_devices = torch.cuda.device_count()\n",
    "#     model = nn.DataParallel(model, device_ids=range(num_devices))  # Use all GPUs\n",
    "#     logger.info(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "\n",
    "\n",
    "wandb.watch(model)\n",
    "\n",
    "if ADV:\n",
    "    discriminator = AdversarialDiscriminator(\n",
    "        d_model=embsize,\n",
    "        n_cls=num_batch_types,\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e4ea79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = masked_mse_loss\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_dab = nn.CrossEntropyLoss()\n",
    "#### OPTIMIZER ####\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=lr, eps=1e-4 if config.amp else 1e-8\n",
    ")\n",
    "\n",
    "\n",
    "# optimizer = torch.optim.AdamW(\n",
    "#     filter(lambda p: p.requires_grad, model.parameters()),  # Only trainable parameters\n",
    "#     lr=lr,\n",
    "#     eps=1e-4 if config.amp else 1e-8\n",
    "# )\n",
    "\n",
    "#### SCHEDULER ####\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, schedule_interval, gamma=config.schedule_ratio\n",
    ")\n",
    "\n",
    "\n",
    "if DAB_separate_optim:\n",
    "    optimizer_dab = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler_dab = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer_dab, schedule_interval, gamma=config.schedule_ratio\n",
    "    )\n",
    "if ADV:\n",
    "    criterion_adv = nn.CrossEntropyLoss()  # consider using label smoothing\n",
    "    optimizer_E = torch.optim.Adam(model.parameters(), lr=lr_ADV)\n",
    "    scheduler_E = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer_E, schedule_interval, gamma=config.schedule_ratio\n",
    "    )\n",
    "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr_ADV)\n",
    "    scheduler_D = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer_D, schedule_interval, gamma=config.schedule_ratio\n",
    "    )\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=config.amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b734269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, loader: DataLoader) -> None:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    (\n",
    "        total_loss,\n",
    "        total_mse,\n",
    "        total_cls,\n",
    "        total_cce,\n",
    "        total_mvc,\n",
    "        total_ecs,\n",
    "        total_dab,\n",
    "        total_adv_E,\n",
    "        total_adv_D,\n",
    "        total_zero_log_prob,\n",
    "        total_mvc_zero_log_prob,\n",
    "    ) = (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n",
    "    total_error = 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    num_batches = len(loader)\n",
    "    for batch, batch_data in enumerate(loader):\n",
    "        input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "        input_values = batch_data[\"values\"].to(device)\n",
    "        target_values = batch_data[\"target_values\"].to(device)\n",
    "        batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "        celltype_labels = batch_data[\"celltype_labels\"].to(device)\n",
    "\n",
    "        src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "        with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                input_values,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n",
    "                CLS=CLS,\n",
    "                CCE=CCE,\n",
    "                MVC=MVC,\n",
    "                ECS=ECS,\n",
    "                do_sample=do_sample_in_train,\n",
    "                #generative_training=False\n",
    "            )\n",
    "\n",
    "            masked_positions = input_values.eq(mask_value)  # the postions to predict\n",
    "            loss = 0.0\n",
    "            metrics_to_log = {}\n",
    "            if MLM:\n",
    "                loss_mse = criterion(\n",
    "                    output_dict[\"mlm_output\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_mse\n",
    "                metrics_to_log = {\"train/mse\": loss_mse.item()}\n",
    "            if explicit_zero_prob:\n",
    "                loss_zero_log_prob = criterion_neg_log_bernoulli(\n",
    "                    output_dict[\"mlm_zero_probs\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_zero_log_prob\n",
    "                metrics_to_log.update({\"train/nzlp\": loss_zero_log_prob.item()})\n",
    "            if CLS:\n",
    "                loss_cls = criterion_cls(output_dict[\"cls_output\"], celltype_labels)\n",
    "                loss = loss + loss_cls\n",
    "                metrics_to_log.update({\"train/cls\": loss_cls.item()})\n",
    "\n",
    "                error_rate = 1 - (\n",
    "                    (output_dict[\"cls_output\"].argmax(1) == celltype_labels)\n",
    "                    .sum()\n",
    "                    .item()\n",
    "                ) / celltype_labels.size(0)\n",
    "            if CCE:\n",
    "                loss_cce = 10 * output_dict[\"loss_cce\"]\n",
    "                loss = loss + loss_cce\n",
    "                metrics_to_log.update({\"train/cce\": loss_cce.item()})\n",
    "            if MVC:\n",
    "                loss_mvc = criterion(\n",
    "                    output_dict[\"mvc_output\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_mvc\n",
    "                metrics_to_log.update({\"train/mvc\": loss_mvc.item()})\n",
    "            if MVC and explicit_zero_prob:\n",
    "                loss_mvc_zero_log_prob = criterion_neg_log_bernoulli(\n",
    "                    output_dict[\"mvc_zero_probs\"], target_values, masked_positions\n",
    "                )\n",
    "                loss = loss + loss_mvc_zero_log_prob\n",
    "                metrics_to_log.update({\"train/mvc_nzlp\": loss_mvc_zero_log_prob.item()})\n",
    "            if ECS:\n",
    "                loss_ecs = 10 * output_dict[\"loss_ecs\"]\n",
    "                loss = loss + loss_ecs\n",
    "                metrics_to_log.update({\"train/ecs\": loss_ecs.item()})\n",
    "            if DAB:\n",
    "                # try weighting and separate optimizer\n",
    "                loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n",
    "                loss = loss + dab_weight * loss_dab\n",
    "                metrics_to_log.update({\"train/dab\": loss_dab.item()})\n",
    "\n",
    "        model.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        with warnings.catch_warnings(record=True) as w:\n",
    "            warnings.filterwarnings(\"always\")\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(),\n",
    "                1.0,\n",
    "                error_if_nonfinite=False if scaler.is_enabled() else True,\n",
    "            )\n",
    "            if len(w) > 0:\n",
    "                logger.warning(\n",
    "                    f\"Found infinite gradient. This may be caused by the gradient \"\n",
    "                    f\"scaler. The current scale is {scaler.get_scale()}. This warning \"\n",
    "                    \"can be ignored if no longer occurs after autoscaling of the scaler.\"\n",
    "                )\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if ADV:\n",
    "            # rerun the model for adversarial training\n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                input_values,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n",
    "                CLS=CLS,\n",
    "                CCE=CCE,\n",
    "                MVC=MVC,\n",
    "                ECS=ECS,\n",
    "                do_sample=do_sample_in_train,\n",
    "                #generative_training=False\n",
    "            )\n",
    "\n",
    "            # TRAINING DISCRIMINATOR\n",
    "            loss_adv_D = criterion_adv(\n",
    "                discriminator(output_dict[\"cell_emb\"].detach()), batch_labels\n",
    "            )\n",
    "            if epoch > adv_D_delay_epochs:\n",
    "                discriminator.zero_grad()\n",
    "                loss_adv_D.backward()\n",
    "                optimizer_D.step()\n",
    "\n",
    "            # TRAINING ENCODER\n",
    "            loss_adv_E = -criterion_adv(\n",
    "                discriminator(output_dict[\"cell_emb\"]), batch_labels\n",
    "            )\n",
    "            # NOTE: the loss is negative here because we want to maximize\n",
    "            # the cross_entropy_loss, in other words, disguise against the discriminator\n",
    "            if epoch > adv_E_delay_epochs:\n",
    "                model.zero_grad()\n",
    "                discriminator.zero_grad()\n",
    "                loss_adv_E.backward()\n",
    "                optimizer_E.step()\n",
    "\n",
    "        wandb.log(metrics_to_log)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_mse += loss_mse.item() if MLM else 0.0\n",
    "        total_cls += loss_cls.item() if CLS else 0.0\n",
    "        total_cce += loss_cce.item() if CCE else 0.0\n",
    "        total_mvc += loss_mvc.item() if MVC else 0.0\n",
    "        total_ecs += loss_ecs.item() if ECS else 0.0\n",
    "        total_dab += loss_dab.item() if DAB else 0.0\n",
    "        total_adv_E += loss_adv_E.item() if ADV else 0.0\n",
    "        total_adv_D += loss_adv_D.item() if ADV else 0.0\n",
    "        total_zero_log_prob += loss_zero_log_prob.item() if explicit_zero_prob else 0.0\n",
    "        total_mvc_zero_log_prob += (\n",
    "            loss_mvc_zero_log_prob.item() if MVC and explicit_zero_prob else 0.0\n",
    "        )\n",
    "        total_error += error_rate\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            cur_mse = total_mse / log_interval\n",
    "            cur_cls = total_cls / log_interval if CLS else 0.0\n",
    "            cur_cce = total_cce / log_interval if CCE else 0.0\n",
    "            cur_mvc = total_mvc / log_interval if MVC else 0.0\n",
    "            cur_ecs = total_ecs / log_interval if ECS else 0.0\n",
    "            cur_dab = total_dab / log_interval if DAB else 0.0\n",
    "            cur_adv_E = total_adv_E / log_interval if ADV else 0.0\n",
    "            cur_adv_D = total_adv_D / log_interval if ADV else 0.0\n",
    "            cur_zero_log_prob = (\n",
    "                total_zero_log_prob / log_interval if explicit_zero_prob else 0.0\n",
    "            )\n",
    "            cur_mvc_zero_log_prob = (\n",
    "                total_mvc_zero_log_prob / log_interval\n",
    "                if MVC and explicit_zero_prob\n",
    "                else 0.0\n",
    "            )\n",
    "            cur_error = total_error / log_interval\n",
    "            # ppl = math.exp(cur_loss)\n",
    "            logger.info(\n",
    "                f\"| epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | \"\n",
    "                f\"lr {lr:05.4f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                f\"loss {cur_loss:5.2f} | \"\n",
    "                + (f\"mse {cur_mse:5.2f} | mre {cur_error:5.2f} |\" if MLM else \"\")\n",
    "                + (f\"cls {cur_cls:5.2f} | \" if CLS else \"\")\n",
    "                + (f\"err {cur_error:5.2f} | \" if CLS else \"\")\n",
    "                + (f\"cce {cur_cce:5.2f} |\" if CCE else \"\")\n",
    "                + (f\"mvc {cur_mvc:5.2f} |\" if MVC else \"\")\n",
    "                + (f\"ecs {cur_ecs:5.2f} |\" if ECS else \"\")\n",
    "                + (f\"dab {cur_dab:5.2f} |\" if DAB else \"\")\n",
    "                + (f\"adv_E {cur_adv_E:5.2f} |\" if ADV else \"\")\n",
    "                + (f\"adv_D {cur_adv_D:5.2f} |\" if ADV else \"\")\n",
    "                + (f\"nzlp {cur_zero_log_prob:5.2f} |\" if explicit_zero_prob else \"\")\n",
    "                + (\n",
    "                    f\"mvc_nzlp {cur_mvc_zero_log_prob:5.2f} |\"\n",
    "                    if MVC and explicit_zero_prob\n",
    "                    else \"\"\n",
    "                )\n",
    "            )\n",
    "            total_loss = 0\n",
    "            total_mse = 0\n",
    "            total_cls = 0\n",
    "            total_cce = 0\n",
    "            total_mvc = 0\n",
    "            total_ecs = 0\n",
    "            total_dab = 0\n",
    "            total_adv_E = 0\n",
    "            total_adv_D = 0\n",
    "            total_zero_log_prob = 0\n",
    "            total_mvc_zero_log_prob = 0\n",
    "            total_error = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def define_wandb_metrics():\n",
    "    wandb.define_metric(\"valid/mse\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/mre\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/dab\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"valid/sum_mse_dab\", summary=\"min\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"test/avg_bio\", summary=\"max\")\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader, return_raw: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_error = 0.0\n",
    "    total_dab = 0.0\n",
    "    total_num = 0\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            target_values = batch_data[\"target_values\"].to(device)\n",
    "            batch_labels = batch_data[\"batch_labels\"].to(device)\n",
    "            celltype_labels = batch_data[\"celltype_labels\"].to(device)\n",
    "\n",
    "            src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "            with torch.cuda.amp.autocast(enabled=config.amp):\n",
    "                output_dict = model(\n",
    "                    input_gene_ids,\n",
    "                    input_values,\n",
    "                    src_key_padding_mask=src_key_padding_mask,\n",
    "                    batch_labels=batch_labels if INPUT_BATCH_LABELS or config.DSBN else None,\n",
    "                    CLS=CLS,  # evaluation does not need CLS or CCE\n",
    "                    CCE=False,\n",
    "                    MVC=False,\n",
    "                    ECS=False,\n",
    "                    do_sample=do_sample_in_train,\n",
    "                    #generative_training = False,\n",
    "                )\n",
    "                output_values = output_dict[\"cls_output\"]\n",
    "                loss = criterion_cls(output_values, celltype_labels)\n",
    "\n",
    "                if DAB:\n",
    "                    loss_dab = criterion_dab(output_dict[\"dab_output\"], batch_labels)\n",
    "\n",
    "            total_loss += loss.item() * len(input_gene_ids)\n",
    "            accuracy = (output_values.argmax(1) == celltype_labels).sum().item()\n",
    "            total_error += (1 - accuracy / len(input_gene_ids)) * len(input_gene_ids)\n",
    "            total_dab += loss_dab.item() * len(input_gene_ids) if DAB else 0.0\n",
    "            total_num += len(input_gene_ids)\n",
    "            preds = output_values.argmax(1).cpu().numpy()\n",
    "            predictions.append(preds)\n",
    "\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"valid/mse\": total_loss / total_num,\n",
    "            \"valid/err\": total_error / total_num,\n",
    "            \"valid/dab\": total_dab / total_num,\n",
    "            \"valid/sum_mse_dab\": (total_loss + dab_weight * total_dab) / total_num,\n",
    "            \"epoch\": epoch,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    if return_raw:\n",
    "        return np.concatenate(predictions, axis=0)\n",
    "\n",
    "    return total_loss / total_num, total_error / total_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734a503c",
   "metadata": {},
   "source": [
    "## Step 4: Finetune scGPT with task-specific objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e48b893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random masking at epoch   1, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   1 | 100/23007 batches | lr 0.0001 | ms/batch 497.32 | loss  3.90 | cls  3.90 | err  0.90 | \n",
      "scGPT - INFO - | epoch   1 | 200/23007 batches | lr 0.0001 | ms/batch 485.66 | loss  3.48 | cls  3.48 | err  0.87 | \n",
      "scGPT - INFO - | epoch   1 | 300/23007 batches | lr 0.0001 | ms/batch 490.48 | loss  3.51 | cls  3.51 | err  0.89 | \n",
      "scGPT - INFO - | epoch   1 | 400/23007 batches | lr 0.0001 | ms/batch 494.03 | loss  3.49 | cls  3.49 | err  0.87 | \n",
      "scGPT - INFO - | epoch   1 | 500/23007 batches | lr 0.0001 | ms/batch 495.11 | loss  3.42 | cls  3.42 | err  0.86 | \n",
      "scGPT - INFO - | epoch   1 | 600/23007 batches | lr 0.0001 | ms/batch 496.51 | loss  3.41 | cls  3.41 | err  0.87 | \n",
      "scGPT - INFO - | epoch   1 | 700/23007 batches | lr 0.0001 | ms/batch 496.13 | loss  3.32 | cls  3.32 | err  0.82 | \n",
      "scGPT - INFO - | epoch   1 | 800/23007 batches | lr 0.0001 | ms/batch 496.33 | loss  3.06 | cls  3.06 | err  0.69 | \n",
      "scGPT - INFO - | epoch   1 | 900/23007 batches | lr 0.0001 | ms/batch 496.67 | loss  2.95 | cls  2.95 | err  0.69 | \n",
      "scGPT - INFO - | epoch   1 | 1000/23007 batches | lr 0.0001 | ms/batch 497.16 | loss  2.77 | cls  2.77 | err  0.65 | \n",
      "scGPT - INFO - | epoch   1 | 1100/23007 batches | lr 0.0001 | ms/batch 496.82 | loss  2.89 | cls  2.89 | err  0.68 | \n",
      "scGPT - INFO - | epoch   1 | 1200/23007 batches | lr 0.0001 | ms/batch 496.70 | loss  2.76 | cls  2.76 | err  0.66 | \n",
      "scGPT - INFO - | epoch   1 | 1300/23007 batches | lr 0.0001 | ms/batch 496.66 | loss  2.66 | cls  2.66 | err  0.63 | \n",
      "scGPT - INFO - | epoch   1 | 1400/23007 batches | lr 0.0001 | ms/batch 496.57 | loss  2.58 | cls  2.58 | err  0.63 | \n",
      "scGPT - INFO - | epoch   1 | 1500/23007 batches | lr 0.0001 | ms/batch 496.86 | loss  2.61 | cls  2.61 | err  0.62 | \n",
      "scGPT - INFO - | epoch   1 | 1600/23007 batches | lr 0.0001 | ms/batch 496.66 | loss  2.51 | cls  2.51 | err  0.59 | \n",
      "scGPT - INFO - | epoch   1 | 1700/23007 batches | lr 0.0001 | ms/batch 496.85 | loss  2.49 | cls  2.49 | err  0.59 | \n",
      "scGPT - INFO - | epoch   1 | 1800/23007 batches | lr 0.0001 | ms/batch 497.05 | loss  2.40 | cls  2.40 | err  0.58 | \n",
      "scGPT - INFO - | epoch   1 | 1900/23007 batches | lr 0.0001 | ms/batch 496.93 | loss  2.32 | cls  2.32 | err  0.56 | \n",
      "scGPT - INFO - | epoch   1 | 2000/23007 batches | lr 0.0001 | ms/batch 497.49 | loss  2.43 | cls  2.43 | err  0.59 | \n",
      "scGPT - INFO - | epoch   1 | 2100/23007 batches | lr 0.0001 | ms/batch 497.02 | loss  2.36 | cls  2.36 | err  0.58 | \n",
      "scGPT - INFO - | epoch   1 | 2200/23007 batches | lr 0.0001 | ms/batch 497.01 | loss  2.24 | cls  2.24 | err  0.54 | \n",
      "scGPT - INFO - | epoch   1 | 2300/23007 batches | lr 0.0001 | ms/batch 497.02 | loss  2.33 | cls  2.33 | err  0.56 | \n",
      "scGPT - INFO - | epoch   1 | 2400/23007 batches | lr 0.0001 | ms/batch 496.89 | loss  2.19 | cls  2.19 | err  0.55 | \n",
      "scGPT - INFO - | epoch   1 | 2500/23007 batches | lr 0.0001 | ms/batch 496.71 | loss  2.19 | cls  2.19 | err  0.53 | \n",
      "scGPT - INFO - | epoch   1 | 2600/23007 batches | lr 0.0001 | ms/batch 496.99 | loss  2.24 | cls  2.24 | err  0.53 | \n",
      "scGPT - INFO - | epoch   1 | 2700/23007 batches | lr 0.0001 | ms/batch 497.13 | loss  2.30 | cls  2.30 | err  0.56 | \n",
      "scGPT - INFO - | epoch   1 | 2800/23007 batches | lr 0.0001 | ms/batch 497.22 | loss  2.18 | cls  2.18 | err  0.53 | \n",
      "scGPT - INFO - | epoch   1 | 2900/23007 batches | lr 0.0001 | ms/batch 496.89 | loss  2.19 | cls  2.19 | err  0.55 | \n",
      "scGPT - INFO - | epoch   1 | 3000/23007 batches | lr 0.0001 | ms/batch 497.46 | loss  2.08 | cls  2.08 | err  0.52 | \n",
      "scGPT - INFO - | epoch   1 | 3100/23007 batches | lr 0.0001 | ms/batch 497.00 | loss  2.13 | cls  2.13 | err  0.52 | \n",
      "scGPT - INFO - | epoch   1 | 3200/23007 batches | lr 0.0001 | ms/batch 497.13 | loss  2.07 | cls  2.07 | err  0.52 | \n",
      "scGPT - INFO - | epoch   1 | 3300/23007 batches | lr 0.0001 | ms/batch 497.11 | loss  2.07 | cls  2.07 | err  0.49 | \n",
      "scGPT - INFO - | epoch   1 | 3400/23007 batches | lr 0.0001 | ms/batch 497.18 | loss  2.06 | cls  2.06 | err  0.49 | \n",
      "scGPT - INFO - | epoch   1 | 3500/23007 batches | lr 0.0001 | ms/batch 496.89 | loss  2.03 | cls  2.03 | err  0.48 | \n",
      "scGPT - INFO - | epoch   1 | 3600/23007 batches | lr 0.0001 | ms/batch 497.04 | loss  1.93 | cls  1.93 | err  0.45 | \n",
      "scGPT - INFO - | epoch   1 | 3700/23007 batches | lr 0.0001 | ms/batch 497.06 | loss  1.94 | cls  1.94 | err  0.44 | \n",
      "scGPT - INFO - | epoch   1 | 3800/23007 batches | lr 0.0001 | ms/batch 496.91 | loss  1.86 | cls  1.86 | err  0.44 | \n",
      "scGPT - INFO - | epoch   1 | 3900/23007 batches | lr 0.0001 | ms/batch 496.85 | loss  1.85 | cls  1.85 | err  0.44 | \n",
      "scGPT - INFO - | epoch   1 | 4000/23007 batches | lr 0.0001 | ms/batch 497.32 | loss  1.81 | cls  1.81 | err  0.43 | \n",
      "scGPT - INFO - | epoch   1 | 4100/23007 batches | lr 0.0001 | ms/batch 496.98 | loss  1.82 | cls  1.82 | err  0.44 | \n",
      "scGPT - INFO - | epoch   1 | 4200/23007 batches | lr 0.0001 | ms/batch 496.92 | loss  1.82 | cls  1.82 | err  0.43 | \n",
      "scGPT - INFO - | epoch   1 | 4300/23007 batches | lr 0.0001 | ms/batch 496.78 | loss  1.79 | cls  1.79 | err  0.44 | \n",
      "scGPT - INFO - | epoch   1 | 4400/23007 batches | lr 0.0001 | ms/batch 496.87 | loss  1.74 | cls  1.74 | err  0.39 | \n",
      "scGPT - INFO - | epoch   1 | 4500/23007 batches | lr 0.0001 | ms/batch 496.78 | loss  1.77 | cls  1.77 | err  0.41 | \n",
      "scGPT - INFO - | epoch   1 | 4600/23007 batches | lr 0.0001 | ms/batch 497.14 | loss  1.86 | cls  1.86 | err  0.44 | \n",
      "scGPT - INFO - | epoch   1 | 4700/23007 batches | lr 0.0001 | ms/batch 496.91 | loss  1.82 | cls  1.82 | err  0.43 | \n",
      "scGPT - INFO - | epoch   1 | 4800/23007 batches | lr 0.0001 | ms/batch 497.04 | loss  1.78 | cls  1.78 | err  0.41 | \n",
      "scGPT - INFO - | epoch   1 | 4900/23007 batches | lr 0.0001 | ms/batch 496.85 | loss  1.84 | cls  1.84 | err  0.42 | \n",
      "scGPT - INFO - | epoch   1 | 5000/23007 batches | lr 0.0001 | ms/batch 497.63 | loss  1.91 | cls  1.91 | err  0.46 | \n",
      "scGPT - INFO - | epoch   1 | 5100/23007 batches | lr 0.0001 | ms/batch 497.15 | loss  1.70 | cls  1.70 | err  0.38 | \n",
      "scGPT - INFO - | epoch   1 | 5200/23007 batches | lr 0.0001 | ms/batch 497.45 | loss  1.77 | cls  1.77 | err  0.41 | \n",
      "scGPT - INFO - | epoch   1 | 5300/23007 batches | lr 0.0001 | ms/batch 497.37 | loss  1.69 | cls  1.69 | err  0.41 | \n",
      "scGPT - INFO - | epoch   1 | 5400/23007 batches | lr 0.0001 | ms/batch 497.11 | loss  1.67 | cls  1.67 | err  0.40 | \n",
      "scGPT - INFO - | epoch   1 | 5500/23007 batches | lr 0.0001 | ms/batch 497.17 | loss  1.67 | cls  1.67 | err  0.41 | \n",
      "scGPT - INFO - | epoch   1 | 5600/23007 batches | lr 0.0001 | ms/batch 497.00 | loss  1.71 | cls  1.71 | err  0.42 | \n",
      "scGPT - INFO - | epoch   1 | 5700/23007 batches | lr 0.0001 | ms/batch 496.99 | loss  1.72 | cls  1.72 | err  0.41 | \n",
      "scGPT - INFO - | epoch   1 | 5800/23007 batches | lr 0.0001 | ms/batch 496.94 | loss  1.73 | cls  1.73 | err  0.41 | \n",
      "scGPT - INFO - | epoch   1 | 5900/23007 batches | lr 0.0001 | ms/batch 497.11 | loss  1.56 | cls  1.56 | err  0.37 | \n",
      "scGPT - INFO - | epoch   1 | 6000/23007 batches | lr 0.0001 | ms/batch 497.96 | loss  1.68 | cls  1.68 | err  0.41 | \n",
      "scGPT - INFO - | epoch   1 | 6100/23007 batches | lr 0.0001 | ms/batch 496.83 | loss  1.63 | cls  1.63 | err  0.41 | \n",
      "scGPT - INFO - | epoch   1 | 6200/23007 batches | lr 0.0001 | ms/batch 496.85 | loss  1.70 | cls  1.70 | err  0.41 | \n",
      "scGPT - INFO - | epoch   1 | 6300/23007 batches | lr 0.0001 | ms/batch 496.98 | loss  1.64 | cls  1.64 | err  0.42 | \n",
      "scGPT - INFO - | epoch   1 | 6400/23007 batches | lr 0.0001 | ms/batch 497.12 | loss  1.59 | cls  1.59 | err  0.39 | \n",
      "scGPT - INFO - | epoch   1 | 6500/23007 batches | lr 0.0001 | ms/batch 497.04 | loss  1.53 | cls  1.53 | err  0.38 | \n",
      "scGPT - INFO - | epoch   1 | 6600/23007 batches | lr 0.0001 | ms/batch 497.57 | loss  1.63 | cls  1.63 | err  0.39 | \n",
      "scGPT - INFO - | epoch   1 | 6700/23007 batches | lr 0.0001 | ms/batch 497.09 | loss  1.71 | cls  1.71 | err  0.41 | \n",
      "scGPT - INFO - | epoch   1 | 6800/23007 batches | lr 0.0001 | ms/batch 497.09 | loss  1.67 | cls  1.67 | err  0.41 | \n",
      "scGPT - INFO - | epoch   1 | 6900/23007 batches | lr 0.0001 | ms/batch 497.15 | loss  1.58 | cls  1.58 | err  0.39 | \n",
      "scGPT - INFO - | epoch   1 | 7000/23007 batches | lr 0.0001 | ms/batch 497.72 | loss  1.49 | cls  1.49 | err  0.39 | \n",
      "scGPT - INFO - | epoch   1 | 7100/23007 batches | lr 0.0001 | ms/batch 497.63 | loss  1.69 | cls  1.69 | err  0.41 | \n",
      "scGPT - INFO - | epoch   1 | 7200/23007 batches | lr 0.0001 | ms/batch 497.34 | loss  1.68 | cls  1.68 | err  0.42 | \n",
      "scGPT - INFO - | epoch   1 | 7300/23007 batches | lr 0.0001 | ms/batch 497.13 | loss  1.63 | cls  1.63 | err  0.38 | \n",
      "scGPT - INFO - | epoch   1 | 7400/23007 batches | lr 0.0001 | ms/batch 497.06 | loss  1.60 | cls  1.60 | err  0.40 | \n",
      "scGPT - INFO - | epoch   1 | 7500/23007 batches | lr 0.0001 | ms/batch 496.90 | loss  1.57 | cls  1.57 | err  0.38 | \n",
      "scGPT - INFO - | epoch   1 | 7600/23007 batches | lr 0.0001 | ms/batch 497.02 | loss  1.51 | cls  1.51 | err  0.38 | \n",
      "scGPT - INFO - | epoch   1 | 7700/23007 batches | lr 0.0001 | ms/batch 497.23 | loss  1.58 | cls  1.58 | err  0.39 | \n",
      "scGPT - INFO - | epoch   1 | 7800/23007 batches | lr 0.0001 | ms/batch 497.00 | loss  1.60 | cls  1.60 | err  0.41 | \n",
      "scGPT - INFO - | epoch   1 | 7900/23007 batches | lr 0.0001 | ms/batch 496.63 | loss  1.46 | cls  1.46 | err  0.35 | \n",
      "scGPT - INFO - | epoch   1 | 8000/23007 batches | lr 0.0001 | ms/batch 497.37 | loss  1.46 | cls  1.46 | err  0.36 | \n",
      "scGPT - INFO - | epoch   1 | 8100/23007 batches | lr 0.0001 | ms/batch 496.90 | loss  1.59 | cls  1.59 | err  0.39 | \n",
      "scGPT - INFO - | epoch   1 | 8200/23007 batches | lr 0.0001 | ms/batch 497.11 | loss  1.61 | cls  1.61 | err  0.39 | \n",
      "scGPT - INFO - | epoch   1 | 8300/23007 batches | lr 0.0001 | ms/batch 497.03 | loss  1.54 | cls  1.54 | err  0.39 | \n",
      "scGPT - INFO - | epoch   1 | 8400/23007 batches | lr 0.0001 | ms/batch 497.04 | loss  1.50 | cls  1.50 | err  0.38 | \n",
      "scGPT - INFO - | epoch   1 | 8500/23007 batches | lr 0.0001 | ms/batch 496.95 | loss  1.53 | cls  1.53 | err  0.38 | \n",
      "scGPT - INFO - | epoch   1 | 8600/23007 batches | lr 0.0001 | ms/batch 497.02 | loss  1.46 | cls  1.46 | err  0.36 | \n",
      "scGPT - INFO - | epoch   1 | 8700/23007 batches | lr 0.0001 | ms/batch 497.08 | loss  1.52 | cls  1.52 | err  0.38 | \n",
      "scGPT - INFO - | epoch   1 | 8800/23007 batches | lr 0.0001 | ms/batch 497.20 | loss  1.46 | cls  1.46 | err  0.37 | \n",
      "scGPT - INFO - | epoch   1 | 8900/23007 batches | lr 0.0001 | ms/batch 497.12 | loss  1.52 | cls  1.52 | err  0.37 | \n",
      "scGPT - INFO - | epoch   1 | 9000/23007 batches | lr 0.0001 | ms/batch 497.63 | loss  1.47 | cls  1.47 | err  0.37 | \n",
      "scGPT - INFO - | epoch   1 | 9100/23007 batches | lr 0.0001 | ms/batch 496.90 | loss  1.43 | cls  1.43 | err  0.37 | \n",
      "scGPT - INFO - | epoch   1 | 9200/23007 batches | lr 0.0001 | ms/batch 496.92 | loss  1.39 | cls  1.39 | err  0.33 | \n",
      "scGPT - INFO - | epoch   1 | 9300/23007 batches | lr 0.0001 | ms/batch 497.15 | loss  1.46 | cls  1.46 | err  0.37 | \n",
      "scGPT - INFO - | epoch   1 | 9400/23007 batches | lr 0.0001 | ms/batch 497.28 | loss  1.45 | cls  1.45 | err  0.38 | \n",
      "scGPT - INFO - | epoch   1 | 9500/23007 batches | lr 0.0001 | ms/batch 497.03 | loss  1.42 | cls  1.42 | err  0.35 | \n",
      "scGPT - INFO - | epoch   1 | 9600/23007 batches | lr 0.0001 | ms/batch 496.93 | loss  1.49 | cls  1.49 | err  0.40 | \n",
      "scGPT - INFO - | epoch   1 | 9700/23007 batches | lr 0.0001 | ms/batch 497.00 | loss  1.43 | cls  1.43 | err  0.38 | \n",
      "scGPT - INFO - | epoch   1 | 9800/23007 batches | lr 0.0001 | ms/batch 497.06 | loss  1.39 | cls  1.39 | err  0.34 | \n",
      "scGPT - INFO - | epoch   1 | 9900/23007 batches | lr 0.0001 | ms/batch 497.17 | loss  1.42 | cls  1.42 | err  0.35 | \n",
      "scGPT - INFO - | epoch   1 | 10000/23007 batches | lr 0.0001 | ms/batch 497.69 | loss  1.40 | cls  1.40 | err  0.35 | \n",
      "scGPT - INFO - | epoch   1 | 10100/23007 batches | lr 0.0001 | ms/batch 497.26 | loss  1.41 | cls  1.41 | err  0.34 | \n",
      "scGPT - INFO - | epoch   1 | 10200/23007 batches | lr 0.0001 | ms/batch 497.20 | loss  1.41 | cls  1.41 | err  0.36 | \n",
      "scGPT - INFO - | epoch   1 | 10300/23007 batches | lr 0.0001 | ms/batch 497.30 | loss  1.48 | cls  1.48 | err  0.37 | \n",
      "scGPT - INFO - | epoch   1 | 10400/23007 batches | lr 0.0001 | ms/batch 497.48 | loss  1.36 | cls  1.36 | err  0.33 | \n",
      "scGPT - INFO - | epoch   1 | 10500/23007 batches | lr 0.0001 | ms/batch 497.04 | loss  1.33 | cls  1.33 | err  0.36 | \n",
      "scGPT - INFO - | epoch   1 | 10600/23007 batches | lr 0.0001 | ms/batch 496.99 | loss  1.48 | cls  1.48 | err  0.36 | \n",
      "scGPT - INFO - | epoch   1 | 10700/23007 batches | lr 0.0001 | ms/batch 496.94 | loss  1.29 | cls  1.29 | err  0.33 | \n",
      "scGPT - INFO - | epoch   1 | 10800/23007 batches | lr 0.0001 | ms/batch 496.84 | loss  1.38 | cls  1.38 | err  0.35 | \n",
      "scGPT - INFO - | epoch   1 | 10900/23007 batches | lr 0.0001 | ms/batch 496.95 | loss  1.35 | cls  1.35 | err  0.35 | \n",
      "scGPT - INFO - | epoch   1 | 11000/23007 batches | lr 0.0001 | ms/batch 497.78 | loss  1.29 | cls  1.29 | err  0.32 | \n",
      "scGPT - INFO - | epoch   1 | 11100/23007 batches | lr 0.0001 | ms/batch 497.15 | loss  1.34 | cls  1.34 | err  0.34 | \n",
      "scGPT - INFO - | epoch   1 | 11200/23007 batches | lr 0.0001 | ms/batch 497.09 | loss  1.31 | cls  1.31 | err  0.34 | \n",
      "scGPT - INFO - | epoch   1 | 11300/23007 batches | lr 0.0001 | ms/batch 497.37 | loss  1.39 | cls  1.39 | err  0.35 | \n",
      "scGPT - INFO - | epoch   1 | 11400/23007 batches | lr 0.0001 | ms/batch 497.50 | loss  1.32 | cls  1.32 | err  0.34 | \n",
      "scGPT - INFO - | epoch   1 | 11500/23007 batches | lr 0.0001 | ms/batch 497.44 | loss  1.34 | cls  1.34 | err  0.36 | \n",
      "scGPT - INFO - | epoch   1 | 11600/23007 batches | lr 0.0001 | ms/batch 497.09 | loss  1.35 | cls  1.35 | err  0.35 | \n",
      "scGPT - INFO - | epoch   1 | 11700/23007 batches | lr 0.0001 | ms/batch 497.71 | loss  1.36 | cls  1.36 | err  0.36 | \n",
      "scGPT - INFO - | epoch   1 | 11800/23007 batches | lr 0.0001 | ms/batch 497.70 | loss  1.37 | cls  1.37 | err  0.35 | \n",
      "scGPT - INFO - | epoch   1 | 11900/23007 batches | lr 0.0001 | ms/batch 497.25 | loss  1.29 | cls  1.29 | err  0.35 | \n",
      "scGPT - INFO - | epoch   1 | 12000/23007 batches | lr 0.0001 | ms/batch 497.73 | loss  1.25 | cls  1.25 | err  0.34 | \n",
      "scGPT - INFO - | epoch   1 | 12100/23007 batches | lr 0.0001 | ms/batch 496.90 | loss  1.32 | cls  1.32 | err  0.34 | \n",
      "scGPT - INFO - | epoch   1 | 12200/23007 batches | lr 0.0001 | ms/batch 496.95 | loss  1.28 | cls  1.28 | err  0.31 | \n",
      "scGPT - INFO - | epoch   1 | 12300/23007 batches | lr 0.0001 | ms/batch 496.94 | loss  1.29 | cls  1.29 | err  0.33 | \n",
      "scGPT - INFO - | epoch   1 | 12400/23007 batches | lr 0.0001 | ms/batch 496.88 | loss  1.24 | cls  1.24 | err  0.33 | \n",
      "scGPT - INFO - | epoch   1 | 12500/23007 batches | lr 0.0001 | ms/batch 496.81 | loss  1.32 | cls  1.32 | err  0.34 | \n",
      "scGPT - INFO - | epoch   1 | 12600/23007 batches | lr 0.0001 | ms/batch 497.20 | loss  1.30 | cls  1.30 | err  0.33 | \n",
      "scGPT - INFO - | epoch   1 | 12700/23007 batches | lr 0.0001 | ms/batch 497.09 | loss  1.40 | cls  1.40 | err  0.36 | \n",
      "scGPT - INFO - | epoch   1 | 12800/23007 batches | lr 0.0001 | ms/batch 497.35 | loss  1.34 | cls  1.34 | err  0.34 | \n",
      "scGPT - INFO - | epoch   1 | 12900/23007 batches | lr 0.0001 | ms/batch 497.18 | loss  1.29 | cls  1.29 | err  0.32 | \n",
      "scGPT - INFO - | epoch   1 | 13000/23007 batches | lr 0.0001 | ms/batch 497.39 | loss  1.24 | cls  1.24 | err  0.32 | \n",
      "scGPT - INFO - | epoch   1 | 13100/23007 batches | lr 0.0001 | ms/batch 497.45 | loss  1.27 | cls  1.27 | err  0.33 | \n",
      "scGPT - INFO - | epoch   1 | 13200/23007 batches | lr 0.0001 | ms/batch 497.38 | loss  1.23 | cls  1.23 | err  0.31 | \n",
      "scGPT - INFO - | epoch   1 | 13300/23007 batches | lr 0.0001 | ms/batch 497.30 | loss  1.17 | cls  1.17 | err  0.30 | \n",
      "scGPT - INFO - | epoch   1 | 13400/23007 batches | lr 0.0001 | ms/batch 497.28 | loss  1.31 | cls  1.31 | err  0.34 | \n",
      "scGPT - INFO - | epoch   1 | 13500/23007 batches | lr 0.0001 | ms/batch 497.03 | loss  1.18 | cls  1.18 | err  0.29 | \n",
      "scGPT - INFO - | epoch   1 | 13600/23007 batches | lr 0.0001 | ms/batch 496.98 | loss  1.25 | cls  1.25 | err  0.31 | \n",
      "scGPT - INFO - | epoch   1 | 13700/23007 batches | lr 0.0001 | ms/batch 497.19 | loss  1.25 | cls  1.25 | err  0.32 | \n",
      "scGPT - INFO - | epoch   1 | 13800/23007 batches | lr 0.0001 | ms/batch 496.96 | loss  1.22 | cls  1.22 | err  0.30 | \n",
      "scGPT - INFO - | epoch   1 | 13900/23007 batches | lr 0.0001 | ms/batch 497.43 | loss  1.28 | cls  1.28 | err  0.33 | \n",
      "scGPT - INFO - | epoch   1 | 14000/23007 batches | lr 0.0001 | ms/batch 497.66 | loss  1.30 | cls  1.30 | err  0.33 | \n",
      "scGPT - INFO - | epoch   1 | 14100/23007 batches | lr 0.0001 | ms/batch 496.74 | loss  1.21 | cls  1.21 | err  0.31 | \n",
      "scGPT - INFO - | epoch   1 | 14200/23007 batches | lr 0.0001 | ms/batch 497.17 | loss  1.29 | cls  1.29 | err  0.34 | \n",
      "scGPT - INFO - | epoch   1 | 14300/23007 batches | lr 0.0001 | ms/batch 497.03 | loss  1.26 | cls  1.26 | err  0.33 | \n",
      "scGPT - INFO - | epoch   1 | 14400/23007 batches | lr 0.0001 | ms/batch 497.14 | loss  1.23 | cls  1.23 | err  0.33 | \n",
      "scGPT - INFO - | epoch   1 | 14500/23007 batches | lr 0.0001 | ms/batch 496.98 | loss  1.28 | cls  1.28 | err  0.33 | \n",
      "scGPT - INFO - | epoch   1 | 14600/23007 batches | lr 0.0001 | ms/batch 497.06 | loss  1.24 | cls  1.24 | err  0.32 | \n",
      "scGPT - INFO - | epoch   1 | 14700/23007 batches | lr 0.0001 | ms/batch 497.04 | loss  1.24 | cls  1.24 | err  0.31 | \n",
      "scGPT - INFO - | epoch   1 | 14800/23007 batches | lr 0.0001 | ms/batch 497.16 | loss  1.22 | cls  1.22 | err  0.31 | \n",
      "scGPT - INFO - | epoch   1 | 14900/23007 batches | lr 0.0001 | ms/batch 497.20 | loss  1.27 | cls  1.27 | err  0.31 | \n",
      "scGPT - INFO - | epoch   1 | 15000/23007 batches | lr 0.0001 | ms/batch 497.46 | loss  1.17 | cls  1.17 | err  0.29 | \n",
      "scGPT - INFO - | epoch   1 | 15100/23007 batches | lr 0.0001 | ms/batch 497.04 | loss  1.19 | cls  1.19 | err  0.31 | \n",
      "scGPT - INFO - | epoch   1 | 15200/23007 batches | lr 0.0001 | ms/batch 497.30 | loss  1.24 | cls  1.24 | err  0.32 | \n",
      "scGPT - INFO - | epoch   1 | 15300/23007 batches | lr 0.0001 | ms/batch 496.87 | loss  1.14 | cls  1.14 | err  0.28 | \n",
      "scGPT - INFO - | epoch   1 | 15400/23007 batches | lr 0.0001 | ms/batch 496.95 | loss  1.23 | cls  1.23 | err  0.32 | \n",
      "scGPT - INFO - | epoch   1 | 15500/23007 batches | lr 0.0001 | ms/batch 496.92 | loss  1.23 | cls  1.23 | err  0.32 | \n",
      "scGPT - INFO - | epoch   1 | 15600/23007 batches | lr 0.0001 | ms/batch 497.05 | loss  1.25 | cls  1.25 | err  0.31 | \n",
      "scGPT - INFO - | epoch   1 | 15700/23007 batches | lr 0.0001 | ms/batch 496.87 | loss  1.19 | cls  1.19 | err  0.30 | \n",
      "scGPT - INFO - | epoch   1 | 15800/23007 batches | lr 0.0001 | ms/batch 497.00 | loss  1.18 | cls  1.18 | err  0.31 | \n",
      "scGPT - INFO - | epoch   1 | 15900/23007 batches | lr 0.0001 | ms/batch 497.18 | loss  1.20 | cls  1.20 | err  0.31 | \n",
      "scGPT - INFO - | epoch   1 | 16000/23007 batches | lr 0.0001 | ms/batch 497.71 | loss  1.08 | cls  1.08 | err  0.27 | \n",
      "scGPT - INFO - | epoch   1 | 16100/23007 batches | lr 0.0001 | ms/batch 496.89 | loss  1.12 | cls  1.12 | err  0.28 | \n",
      "scGPT - INFO - | epoch   1 | 16200/23007 batches | lr 0.0001 | ms/batch 496.94 | loss  1.06 | cls  1.06 | err  0.29 | \n",
      "scGPT - INFO - | epoch   1 | 16300/23007 batches | lr 0.0001 | ms/batch 497.01 | loss  1.11 | cls  1.11 | err  0.28 | \n",
      "scGPT - INFO - | epoch   1 | 16400/23007 batches | lr 0.0001 | ms/batch 497.53 | loss  1.16 | cls  1.16 | err  0.29 | \n",
      "scGPT - INFO - | epoch   1 | 16500/23007 batches | lr 0.0001 | ms/batch 497.29 | loss  1.15 | cls  1.15 | err  0.29 | \n",
      "scGPT - INFO - | epoch   1 | 16600/23007 batches | lr 0.0001 | ms/batch 497.13 | loss  1.17 | cls  1.17 | err  0.30 | \n",
      "scGPT - INFO - | epoch   1 | 16700/23007 batches | lr 0.0001 | ms/batch 496.75 | loss  1.14 | cls  1.14 | err  0.28 | \n",
      "scGPT - INFO - | epoch   1 | 16800/23007 batches | lr 0.0001 | ms/batch 497.19 | loss  1.12 | cls  1.12 | err  0.27 | \n",
      "scGPT - INFO - | epoch   1 | 16900/23007 batches | lr 0.0001 | ms/batch 496.91 | loss  1.20 | cls  1.20 | err  0.30 | \n",
      "scGPT - INFO - | epoch   1 | 17000/23007 batches | lr 0.0001 | ms/batch 497.97 | loss  1.15 | cls  1.15 | err  0.29 | \n",
      "scGPT - INFO - | epoch   1 | 17100/23007 batches | lr 0.0001 | ms/batch 497.24 | loss  1.21 | cls  1.21 | err  0.29 | \n",
      "scGPT - INFO - | epoch   1 | 17200/23007 batches | lr 0.0001 | ms/batch 497.33 | loss  1.09 | cls  1.09 | err  0.28 | \n",
      "scGPT - INFO - | epoch   1 | 17300/23007 batches | lr 0.0001 | ms/batch 497.18 | loss  1.19 | cls  1.19 | err  0.31 | \n",
      "scGPT - INFO - | epoch   1 | 17400/23007 batches | lr 0.0001 | ms/batch 496.64 | loss  1.15 | cls  1.15 | err  0.28 | \n",
      "scGPT - INFO - | epoch   1 | 17500/23007 batches | lr 0.0001 | ms/batch 497.16 | loss  1.09 | cls  1.09 | err  0.28 | \n",
      "scGPT - INFO - | epoch   1 | 17600/23007 batches | lr 0.0001 | ms/batch 497.09 | loss  1.11 | cls  1.11 | err  0.26 | \n",
      "scGPT - INFO - | epoch   1 | 17700/23007 batches | lr 0.0001 | ms/batch 497.06 | loss  1.04 | cls  1.04 | err  0.26 | \n",
      "scGPT - INFO - | epoch   1 | 17800/23007 batches | lr 0.0001 | ms/batch 496.97 | loss  1.18 | cls  1.18 | err  0.30 | \n",
      "scGPT - INFO - | epoch   1 | 17900/23007 batches | lr 0.0001 | ms/batch 496.91 | loss  1.13 | cls  1.13 | err  0.27 | \n",
      "scGPT - INFO - | epoch   1 | 18000/23007 batches | lr 0.0001 | ms/batch 497.19 | loss  1.11 | cls  1.11 | err  0.27 | \n",
      "scGPT - INFO - | epoch   1 | 18100/23007 batches | lr 0.0001 | ms/batch 496.78 | loss  1.19 | cls  1.19 | err  0.31 | \n",
      "scGPT - INFO - | epoch   1 | 18200/23007 batches | lr 0.0001 | ms/batch 496.68 | loss  1.15 | cls  1.15 | err  0.29 | \n",
      "scGPT - INFO - | epoch   1 | 18300/23007 batches | lr 0.0001 | ms/batch 496.65 | loss  1.15 | cls  1.15 | err  0.28 | \n",
      "scGPT - INFO - | epoch   1 | 18400/23007 batches | lr 0.0001 | ms/batch 496.79 | loss  1.22 | cls  1.22 | err  0.31 | \n",
      "scGPT - INFO - | epoch   1 | 18500/23007 batches | lr 0.0001 | ms/batch 496.72 | loss  1.01 | cls  1.01 | err  0.26 | \n",
      "scGPT - INFO - | epoch   1 | 18600/23007 batches | lr 0.0001 | ms/batch 496.82 | loss  1.12 | cls  1.12 | err  0.27 | \n",
      "scGPT - INFO - | epoch   1 | 18700/23007 batches | lr 0.0001 | ms/batch 496.58 | loss  1.13 | cls  1.13 | err  0.28 | \n",
      "scGPT - INFO - | epoch   1 | 18800/23007 batches | lr 0.0001 | ms/batch 496.59 | loss  1.06 | cls  1.06 | err  0.26 | \n",
      "scGPT - INFO - | epoch   1 | 18900/23007 batches | lr 0.0001 | ms/batch 496.53 | loss  1.14 | cls  1.14 | err  0.28 | \n",
      "scGPT - INFO - | epoch   1 | 19000/23007 batches | lr 0.0001 | ms/batch 497.27 | loss  1.15 | cls  1.15 | err  0.27 | \n",
      "scGPT - INFO - | epoch   1 | 19100/23007 batches | lr 0.0001 | ms/batch 496.76 | loss  1.09 | cls  1.09 | err  0.25 | \n",
      "scGPT - INFO - | epoch   1 | 19200/23007 batches | lr 0.0001 | ms/batch 496.97 | loss  1.15 | cls  1.15 | err  0.28 | \n",
      "scGPT - INFO - | epoch   1 | 19300/23007 batches | lr 0.0001 | ms/batch 496.84 | loss  1.15 | cls  1.15 | err  0.28 | \n",
      "scGPT - INFO - | epoch   1 | 19400/23007 batches | lr 0.0001 | ms/batch 496.84 | loss  1.15 | cls  1.15 | err  0.28 | \n",
      "scGPT - INFO - | epoch   1 | 19500/23007 batches | lr 0.0001 | ms/batch 496.76 | loss  1.12 | cls  1.12 | err  0.27 | \n",
      "scGPT - INFO - | epoch   1 | 19600/23007 batches | lr 0.0001 | ms/batch 496.73 | loss  1.04 | cls  1.04 | err  0.26 | \n",
      "scGPT - INFO - | epoch   1 | 19700/23007 batches | lr 0.0001 | ms/batch 496.84 | loss  1.09 | cls  1.09 | err  0.28 | \n",
      "scGPT - INFO - | epoch   1 | 19800/23007 batches | lr 0.0001 | ms/batch 496.55 | loss  1.13 | cls  1.13 | err  0.27 | \n",
      "scGPT - INFO - | epoch   1 | 19900/23007 batches | lr 0.0001 | ms/batch 496.87 | loss  1.15 | cls  1.15 | err  0.28 | \n",
      "scGPT - INFO - | epoch   1 | 20000/23007 batches | lr 0.0001 | ms/batch 497.40 | loss  1.18 | cls  1.18 | err  0.28 | \n",
      "scGPT - INFO - | epoch   1 | 20100/23007 batches | lr 0.0001 | ms/batch 496.87 | loss  1.08 | cls  1.08 | err  0.27 | \n",
      "scGPT - INFO - | epoch   1 | 20200/23007 batches | lr 0.0001 | ms/batch 497.03 | loss  1.20 | cls  1.20 | err  0.30 | \n",
      "scGPT - INFO - | epoch   1 | 20300/23007 batches | lr 0.0001 | ms/batch 496.89 | loss  1.16 | cls  1.16 | err  0.30 | \n",
      "scGPT - INFO - | epoch   1 | 20400/23007 batches | lr 0.0001 | ms/batch 496.78 | loss  1.14 | cls  1.14 | err  0.27 | \n",
      "scGPT - INFO - | epoch   1 | 20500/23007 batches | lr 0.0001 | ms/batch 496.95 | loss  1.12 | cls  1.12 | err  0.28 | \n",
      "scGPT - INFO - | epoch   1 | 20600/23007 batches | lr 0.0001 | ms/batch 497.24 | loss  1.09 | cls  1.09 | err  0.26 | \n",
      "scGPT - INFO - | epoch   1 | 20700/23007 batches | lr 0.0001 | ms/batch 497.48 | loss  1.08 | cls  1.08 | err  0.26 | \n",
      "scGPT - INFO - | epoch   1 | 20800/23007 batches | lr 0.0001 | ms/batch 497.12 | loss  1.08 | cls  1.08 | err  0.28 | \n",
      "scGPT - INFO - | epoch   1 | 20900/23007 batches | lr 0.0001 | ms/batch 497.11 | loss  1.12 | cls  1.12 | err  0.28 | \n",
      "scGPT - INFO - | epoch   1 | 21000/23007 batches | lr 0.0001 | ms/batch 497.39 | loss  1.07 | cls  1.07 | err  0.26 | \n",
      "scGPT - INFO - | epoch   1 | 21100/23007 batches | lr 0.0001 | ms/batch 496.87 | loss  1.02 | cls  1.02 | err  0.26 | \n",
      "scGPT - INFO - | epoch   1 | 21200/23007 batches | lr 0.0001 | ms/batch 496.98 | loss  1.10 | cls  1.10 | err  0.28 | \n",
      "scGPT - INFO - | epoch   1 | 21300/23007 batches | lr 0.0001 | ms/batch 497.17 | loss  1.02 | cls  1.02 | err  0.26 | \n",
      "scGPT - INFO - | epoch   1 | 21400/23007 batches | lr 0.0001 | ms/batch 497.19 | loss  1.07 | cls  1.07 | err  0.26 | \n",
      "scGPT - INFO - | epoch   1 | 21500/23007 batches | lr 0.0001 | ms/batch 497.48 | loss  1.15 | cls  1.15 | err  0.28 | \n",
      "scGPT - INFO - | epoch   1 | 21600/23007 batches | lr 0.0001 | ms/batch 497.20 | loss  1.06 | cls  1.06 | err  0.26 | \n",
      "scGPT - INFO - | epoch   1 | 21700/23007 batches | lr 0.0001 | ms/batch 497.11 | loss  1.05 | cls  1.05 | err  0.27 | \n",
      "scGPT - INFO - | epoch   1 | 21800/23007 batches | lr 0.0001 | ms/batch 497.25 | loss  1.04 | cls  1.04 | err  0.27 | \n",
      "scGPT - INFO - | epoch   1 | 21900/23007 batches | lr 0.0001 | ms/batch 497.40 | loss  1.08 | cls  1.08 | err  0.25 | \n",
      "scGPT - INFO - | epoch   1 | 22000/23007 batches | lr 0.0001 | ms/batch 497.64 | loss  1.02 | cls  1.02 | err  0.27 | \n",
      "scGPT - INFO - | epoch   1 | 22100/23007 batches | lr 0.0001 | ms/batch 497.19 | loss  1.12 | cls  1.12 | err  0.28 | \n",
      "scGPT - INFO - | epoch   1 | 22200/23007 batches | lr 0.0001 | ms/batch 497.11 | loss  1.15 | cls  1.15 | err  0.28 | \n",
      "scGPT - INFO - | epoch   1 | 22300/23007 batches | lr 0.0001 | ms/batch 496.93 | loss  1.00 | cls  1.00 | err  0.25 | \n",
      "scGPT - INFO - | epoch   1 | 22400/23007 batches | lr 0.0001 | ms/batch 497.40 | loss  1.06 | cls  1.06 | err  0.26 | \n",
      "scGPT - INFO - | epoch   1 | 22500/23007 batches | lr 0.0001 | ms/batch 497.12 | loss  1.03 | cls  1.03 | err  0.23 | \n",
      "scGPT - INFO - | epoch   1 | 22600/23007 batches | lr 0.0001 | ms/batch 496.96 | loss  1.10 | cls  1.10 | err  0.26 | \n",
      "scGPT - INFO - | epoch   1 | 22700/23007 batches | lr 0.0001 | ms/batch 497.20 | loss  1.02 | cls  1.02 | err  0.25 | \n",
      "scGPT - INFO - | epoch   1 | 22800/23007 batches | lr 0.0001 | ms/batch 497.09 | loss  0.97 | cls  0.97 | err  0.23 | \n",
      "scGPT - INFO - | epoch   1 | 22900/23007 batches | lr 0.0001 | ms/batch 497.28 | loss  1.06 | cls  1.06 | err  0.25 | \n",
      "scGPT - INFO - | epoch   1 | 23000/23007 batches | lr 0.0001 | ms/batch 497.74 | loss  1.06 | cls  1.06 | err  0.26 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   1 | time: 11938.39s | valid loss/mse 1.1096 | err 0.2823\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 1.1096\n",
      "random masking at epoch   2, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   2 | 100/23007 batches | lr 0.0001 | ms/batch 513.87 | loss  0.98 | cls  0.98 | err  0.25 | \n",
      "scGPT - INFO - | epoch   2 | 200/23007 batches | lr 0.0001 | ms/batch 498.28 | loss  1.03 | cls  1.03 | err  0.26 | \n",
      "scGPT - INFO - | epoch   2 | 300/23007 batches | lr 0.0001 | ms/batch 498.77 | loss  0.98 | cls  0.98 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 400/23007 batches | lr 0.0001 | ms/batch 499.45 | loss  1.02 | cls  1.02 | err  0.25 | \n",
      "scGPT - INFO - | epoch   2 | 500/23007 batches | lr 0.0001 | ms/batch 497.84 | loss  1.00 | cls  1.00 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 600/23007 batches | lr 0.0001 | ms/batch 497.81 | loss  0.99 | cls  0.99 | err  0.25 | \n",
      "scGPT - INFO - | epoch   2 | 700/23007 batches | lr 0.0001 | ms/batch 497.83 | loss  0.97 | cls  0.97 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 800/23007 batches | lr 0.0001 | ms/batch 497.70 | loss  1.03 | cls  1.03 | err  0.26 | \n",
      "scGPT - INFO - | epoch   2 | 900/23007 batches | lr 0.0001 | ms/batch 497.96 | loss  1.02 | cls  1.02 | err  0.26 | \n",
      "scGPT - INFO - | epoch   2 | 1000/23007 batches | lr 0.0001 | ms/batch 498.46 | loss  0.92 | cls  0.92 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 1100/23007 batches | lr 0.0001 | ms/batch 498.00 | loss  1.00 | cls  1.00 | err  0.25 | \n",
      "scGPT - INFO - | epoch   2 | 1200/23007 batches | lr 0.0001 | ms/batch 497.89 | loss  1.14 | cls  1.14 | err  0.28 | \n",
      "scGPT - INFO - | epoch   2 | 1300/23007 batches | lr 0.0001 | ms/batch 497.74 | loss  1.01 | cls  1.01 | err  0.25 | \n",
      "scGPT - INFO - | epoch   2 | 1400/23007 batches | lr 0.0001 | ms/batch 497.87 | loss  0.98 | cls  0.98 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 1500/23007 batches | lr 0.0001 | ms/batch 498.19 | loss  1.00 | cls  1.00 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 1600/23007 batches | lr 0.0001 | ms/batch 497.80 | loss  1.02 | cls  1.02 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 1700/23007 batches | lr 0.0001 | ms/batch 497.75 | loss  0.99 | cls  0.99 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 1800/23007 batches | lr 0.0001 | ms/batch 497.97 | loss  1.02 | cls  1.02 | err  0.25 | \n",
      "scGPT - INFO - | epoch   2 | 1900/23007 batches | lr 0.0001 | ms/batch 498.04 | loss  1.01 | cls  1.01 | err  0.25 | \n",
      "scGPT - INFO - | epoch   2 | 2000/23007 batches | lr 0.0001 | ms/batch 499.06 | loss  0.96 | cls  0.96 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 2100/23007 batches | lr 0.0001 | ms/batch 498.10 | loss  1.02 | cls  1.02 | err  0.26 | \n",
      "scGPT - INFO - | epoch   2 | 2200/23007 batches | lr 0.0001 | ms/batch 498.10 | loss  0.99 | cls  0.99 | err  0.26 | \n",
      "scGPT - INFO - | epoch   2 | 2300/23007 batches | lr 0.0001 | ms/batch 497.96 | loss  1.06 | cls  1.06 | err  0.25 | \n",
      "scGPT - INFO - | epoch   2 | 2400/23007 batches | lr 0.0001 | ms/batch 498.05 | loss  0.93 | cls  0.93 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 2500/23007 batches | lr 0.0001 | ms/batch 498.02 | loss  0.93 | cls  0.93 | err  0.21 | \n",
      "scGPT - INFO - | epoch   2 | 2600/23007 batches | lr 0.0001 | ms/batch 498.29 | loss  1.10 | cls  1.10 | err  0.26 | \n",
      "scGPT - INFO - | epoch   2 | 2700/23007 batches | lr 0.0001 | ms/batch 498.21 | loss  1.00 | cls  1.00 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 2800/23007 batches | lr 0.0001 | ms/batch 498.07 | loss  1.03 | cls  1.03 | err  0.25 | \n",
      "scGPT - INFO - | epoch   2 | 2900/23007 batches | lr 0.0001 | ms/batch 497.92 | loss  0.99 | cls  0.99 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 3000/23007 batches | lr 0.0001 | ms/batch 498.45 | loss  0.97 | cls  0.97 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 3100/23007 batches | lr 0.0001 | ms/batch 498.30 | loss  1.01 | cls  1.01 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 3200/23007 batches | lr 0.0001 | ms/batch 498.08 | loss  1.02 | cls  1.02 | err  0.26 | \n",
      "scGPT - INFO - | epoch   2 | 3300/23007 batches | lr 0.0001 | ms/batch 497.97 | loss  1.01 | cls  1.01 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 3400/23007 batches | lr 0.0001 | ms/batch 498.05 | loss  1.00 | cls  1.00 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 3500/23007 batches | lr 0.0001 | ms/batch 497.87 | loss  1.02 | cls  1.02 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 3600/23007 batches | lr 0.0001 | ms/batch 498.02 | loss  0.93 | cls  0.93 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 3700/23007 batches | lr 0.0001 | ms/batch 498.27 | loss  1.07 | cls  1.07 | err  0.26 | \n",
      "scGPT - INFO - | epoch   2 | 3800/23007 batches | lr 0.0001 | ms/batch 498.02 | loss  0.98 | cls  0.98 | err  0.25 | \n",
      "scGPT - INFO - | epoch   2 | 3900/23007 batches | lr 0.0001 | ms/batch 497.90 | loss  1.01 | cls  1.01 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 4000/23007 batches | lr 0.0001 | ms/batch 498.30 | loss  0.95 | cls  0.95 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 4100/23007 batches | lr 0.0001 | ms/batch 497.90 | loss  0.94 | cls  0.94 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 4200/23007 batches | lr 0.0001 | ms/batch 498.07 | loss  1.03 | cls  1.03 | err  0.25 | \n",
      "scGPT - INFO - | epoch   2 | 4300/23007 batches | lr 0.0001 | ms/batch 498.10 | loss  0.94 | cls  0.94 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 4400/23007 batches | lr 0.0001 | ms/batch 498.03 | loss  0.97 | cls  0.97 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 4500/23007 batches | lr 0.0001 | ms/batch 498.08 | loss  0.98 | cls  0.98 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 4600/23007 batches | lr 0.0001 | ms/batch 498.42 | loss  1.03 | cls  1.03 | err  0.26 | \n",
      "scGPT - INFO - | epoch   2 | 4700/23007 batches | lr 0.0001 | ms/batch 498.11 | loss  1.03 | cls  1.03 | err  0.26 | \n",
      "scGPT - INFO - | epoch   2 | 4800/23007 batches | lr 0.0001 | ms/batch 498.28 | loss  1.02 | cls  1.02 | err  0.26 | \n",
      "scGPT - INFO - | epoch   2 | 4900/23007 batches | lr 0.0001 | ms/batch 498.06 | loss  1.03 | cls  1.03 | err  0.25 | \n",
      "scGPT - INFO - | epoch   2 | 5000/23007 batches | lr 0.0001 | ms/batch 498.80 | loss  1.07 | cls  1.07 | err  0.26 | \n",
      "scGPT - INFO - | epoch   2 | 5100/23007 batches | lr 0.0001 | ms/batch 498.33 | loss  0.99 | cls  0.99 | err  0.25 | \n",
      "scGPT - INFO - | epoch   2 | 5200/23007 batches | lr 0.0001 | ms/batch 498.21 | loss  1.03 | cls  1.03 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 5300/23007 batches | lr 0.0001 | ms/batch 498.23 | loss  0.97 | cls  0.97 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 5400/23007 batches | lr 0.0001 | ms/batch 498.03 | loss  0.95 | cls  0.95 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 5500/23007 batches | lr 0.0001 | ms/batch 498.25 | loss  0.97 | cls  0.97 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 5600/23007 batches | lr 0.0001 | ms/batch 497.94 | loss  0.96 | cls  0.96 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 5700/23007 batches | lr 0.0001 | ms/batch 497.86 | loss  0.98 | cls  0.98 | err  0.25 | \n",
      "scGPT - INFO - | epoch   2 | 5800/23007 batches | lr 0.0001 | ms/batch 497.92 | loss  1.05 | cls  1.05 | err  0.25 | \n",
      "scGPT - INFO - | epoch   2 | 5900/23007 batches | lr 0.0001 | ms/batch 497.80 | loss  0.90 | cls  0.90 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 6000/23007 batches | lr 0.0001 | ms/batch 498.80 | loss  0.99 | cls  0.99 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 6100/23007 batches | lr 0.0001 | ms/batch 497.71 | loss  0.99 | cls  0.99 | err  0.25 | \n",
      "scGPT - INFO - | epoch   2 | 6200/23007 batches | lr 0.0001 | ms/batch 497.72 | loss  1.06 | cls  1.06 | err  0.27 | \n",
      "scGPT - INFO - | epoch   2 | 6300/23007 batches | lr 0.0001 | ms/batch 497.89 | loss  1.00 | cls  1.00 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 6400/23007 batches | lr 0.0001 | ms/batch 498.02 | loss  0.94 | cls  0.94 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 6500/23007 batches | lr 0.0001 | ms/batch 497.72 | loss  0.92 | cls  0.92 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 6600/23007 batches | lr 0.0001 | ms/batch 497.98 | loss  1.02 | cls  1.02 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 6700/23007 batches | lr 0.0001 | ms/batch 498.00 | loss  1.05 | cls  1.05 | err  0.27 | \n",
      "scGPT - INFO - | epoch   2 | 6800/23007 batches | lr 0.0001 | ms/batch 497.83 | loss  1.04 | cls  1.04 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 6900/23007 batches | lr 0.0001 | ms/batch 497.59 | loss  0.99 | cls  0.99 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 7000/23007 batches | lr 0.0001 | ms/batch 498.31 | loss  0.90 | cls  0.90 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 7100/23007 batches | lr 0.0001 | ms/batch 497.89 | loss  1.08 | cls  1.08 | err  0.28 | \n",
      "scGPT - INFO - | epoch   2 | 7200/23007 batches | lr 0.0001 | ms/batch 497.49 | loss  1.03 | cls  1.03 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 7300/23007 batches | lr 0.0001 | ms/batch 497.82 | loss  0.96 | cls  0.96 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 7400/23007 batches | lr 0.0001 | ms/batch 497.89 | loss  0.93 | cls  0.93 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 7500/23007 batches | lr 0.0001 | ms/batch 498.14 | loss  1.01 | cls  1.01 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 7600/23007 batches | lr 0.0001 | ms/batch 497.98 | loss  0.95 | cls  0.95 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 7700/23007 batches | lr 0.0001 | ms/batch 497.96 | loss  0.95 | cls  0.95 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 7800/23007 batches | lr 0.0001 | ms/batch 497.68 | loss  1.06 | cls  1.06 | err  0.27 | \n",
      "scGPT - INFO - | epoch   2 | 7900/23007 batches | lr 0.0001 | ms/batch 497.53 | loss  0.91 | cls  0.91 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 8000/23007 batches | lr 0.0001 | ms/batch 498.32 | loss  0.93 | cls  0.93 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 8100/23007 batches | lr 0.0001 | ms/batch 497.71 | loss  0.97 | cls  0.97 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 8200/23007 batches | lr 0.0001 | ms/batch 497.96 | loss  1.05 | cls  1.05 | err  0.26 | \n",
      "scGPT - INFO - | epoch   2 | 8300/23007 batches | lr 0.0001 | ms/batch 498.32 | loss  0.91 | cls  0.91 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 8400/23007 batches | lr 0.0001 | ms/batch 497.86 | loss  0.94 | cls  0.94 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 8500/23007 batches | lr 0.0001 | ms/batch 498.20 | loss  0.95 | cls  0.95 | err  0.25 | \n",
      "scGPT - INFO - | epoch   2 | 8600/23007 batches | lr 0.0001 | ms/batch 498.18 | loss  0.89 | cls  0.89 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 8700/23007 batches | lr 0.0001 | ms/batch 497.90 | loss  1.00 | cls  1.00 | err  0.25 | \n",
      "scGPT - INFO - | epoch   2 | 8800/23007 batches | lr 0.0001 | ms/batch 497.91 | loss  0.97 | cls  0.97 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 8900/23007 batches | lr 0.0001 | ms/batch 498.05 | loss  0.98 | cls  0.98 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 9000/23007 batches | lr 0.0001 | ms/batch 498.25 | loss  0.94 | cls  0.94 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 9100/23007 batches | lr 0.0001 | ms/batch 498.15 | loss  0.93 | cls  0.93 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 9200/23007 batches | lr 0.0001 | ms/batch 497.91 | loss  0.90 | cls  0.90 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 9300/23007 batches | lr 0.0001 | ms/batch 497.91 | loss  0.99 | cls  0.99 | err  0.26 | \n",
      "scGPT - INFO - | epoch   2 | 9400/23007 batches | lr 0.0001 | ms/batch 498.36 | loss  0.95 | cls  0.95 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 9500/23007 batches | lr 0.0001 | ms/batch 497.61 | loss  0.89 | cls  0.89 | err  0.21 | \n",
      "scGPT - INFO - | epoch   2 | 9600/23007 batches | lr 0.0001 | ms/batch 497.64 | loss  0.96 | cls  0.96 | err  0.25 | \n",
      "scGPT - INFO - | epoch   2 | 9700/23007 batches | lr 0.0001 | ms/batch 497.68 | loss  0.95 | cls  0.95 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 9800/23007 batches | lr 0.0001 | ms/batch 498.21 | loss  0.94 | cls  0.94 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 9900/23007 batches | lr 0.0001 | ms/batch 498.04 | loss  0.97 | cls  0.97 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 10000/23007 batches | lr 0.0001 | ms/batch 498.36 | loss  0.93 | cls  0.93 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 10100/23007 batches | lr 0.0001 | ms/batch 497.65 | loss  0.92 | cls  0.92 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 10200/23007 batches | lr 0.0001 | ms/batch 497.84 | loss  0.97 | cls  0.97 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 10300/23007 batches | lr 0.0001 | ms/batch 497.82 | loss  1.02 | cls  1.02 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 10400/23007 batches | lr 0.0001 | ms/batch 497.77 | loss  0.94 | cls  0.94 | err  0.25 | \n",
      "scGPT - INFO - | epoch   2 | 10500/23007 batches | lr 0.0001 | ms/batch 497.51 | loss  0.90 | cls  0.90 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 10600/23007 batches | lr 0.0001 | ms/batch 497.63 | loss  1.05 | cls  1.05 | err  0.26 | \n",
      "scGPT - INFO - | epoch   2 | 10700/23007 batches | lr 0.0001 | ms/batch 497.76 | loss  0.87 | cls  0.87 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 10800/23007 batches | lr 0.0001 | ms/batch 497.77 | loss  0.93 | cls  0.93 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 10900/23007 batches | lr 0.0001 | ms/batch 497.89 | loss  0.91 | cls  0.91 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 11000/23007 batches | lr 0.0001 | ms/batch 498.40 | loss  0.88 | cls  0.88 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 11100/23007 batches | lr 0.0001 | ms/batch 497.67 | loss  0.96 | cls  0.96 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 11200/23007 batches | lr 0.0001 | ms/batch 497.48 | loss  0.87 | cls  0.87 | err  0.21 | \n",
      "scGPT - INFO - | epoch   2 | 11300/23007 batches | lr 0.0001 | ms/batch 497.70 | loss  0.98 | cls  0.98 | err  0.25 | \n",
      "scGPT - INFO - | epoch   2 | 11400/23007 batches | lr 0.0001 | ms/batch 497.68 | loss  0.94 | cls  0.94 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 11500/23007 batches | lr 0.0001 | ms/batch 497.74 | loss  0.91 | cls  0.91 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 11600/23007 batches | lr 0.0001 | ms/batch 497.67 | loss  0.94 | cls  0.94 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 11700/23007 batches | lr 0.0001 | ms/batch 497.94 | loss  0.94 | cls  0.94 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 11800/23007 batches | lr 0.0001 | ms/batch 498.03 | loss  0.91 | cls  0.91 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 11900/23007 batches | lr 0.0001 | ms/batch 497.88 | loss  0.90 | cls  0.90 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 12000/23007 batches | lr 0.0001 | ms/batch 498.55 | loss  0.89 | cls  0.89 | err  0.21 | \n",
      "scGPT - INFO - | epoch   2 | 12100/23007 batches | lr 0.0001 | ms/batch 497.88 | loss  0.94 | cls  0.94 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 12200/23007 batches | lr 0.0001 | ms/batch 498.05 | loss  0.91 | cls  0.91 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 12300/23007 batches | lr 0.0001 | ms/batch 498.06 | loss  0.89 | cls  0.89 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 12400/23007 batches | lr 0.0001 | ms/batch 498.03 | loss  0.87 | cls  0.87 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 12500/23007 batches | lr 0.0001 | ms/batch 497.77 | loss  0.94 | cls  0.94 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 12600/23007 batches | lr 0.0001 | ms/batch 497.84 | loss  0.92 | cls  0.92 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 12700/23007 batches | lr 0.0001 | ms/batch 497.72 | loss  1.01 | cls  1.01 | err  0.26 | \n",
      "scGPT - INFO - | epoch   2 | 12800/23007 batches | lr 0.0001 | ms/batch 497.96 | loss  0.95 | cls  0.95 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 12900/23007 batches | lr 0.0001 | ms/batch 497.85 | loss  0.90 | cls  0.90 | err  0.21 | \n",
      "scGPT - INFO - | epoch   2 | 13000/23007 batches | lr 0.0001 | ms/batch 498.59 | loss  0.90 | cls  0.90 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 13100/23007 batches | lr 0.0001 | ms/batch 497.70 | loss  0.91 | cls  0.91 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 13200/23007 batches | lr 0.0001 | ms/batch 497.60 | loss  0.90 | cls  0.90 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 13300/23007 batches | lr 0.0001 | ms/batch 497.90 | loss  0.87 | cls  0.87 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 13400/23007 batches | lr 0.0001 | ms/batch 497.84 | loss  0.91 | cls  0.91 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 13500/23007 batches | lr 0.0001 | ms/batch 497.85 | loss  0.86 | cls  0.86 | err  0.20 | \n",
      "scGPT - INFO - | epoch   2 | 13600/23007 batches | lr 0.0001 | ms/batch 497.63 | loss  0.91 | cls  0.91 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 13700/23007 batches | lr 0.0001 | ms/batch 497.89 | loss  0.87 | cls  0.87 | err  0.21 | \n",
      "scGPT - INFO - | epoch   2 | 13800/23007 batches | lr 0.0001 | ms/batch 497.81 | loss  0.86 | cls  0.86 | err  0.21 | \n",
      "scGPT - INFO - | epoch   2 | 13900/23007 batches | lr 0.0001 | ms/batch 497.94 | loss  0.94 | cls  0.94 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 14000/23007 batches | lr 0.0001 | ms/batch 498.30 | loss  0.94 | cls  0.94 | err  0.25 | \n",
      "scGPT - INFO - | epoch   2 | 14100/23007 batches | lr 0.0001 | ms/batch 497.50 | loss  0.88 | cls  0.88 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 14200/23007 batches | lr 0.0001 | ms/batch 497.74 | loss  0.94 | cls  0.94 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 14300/23007 batches | lr 0.0001 | ms/batch 497.73 | loss  0.92 | cls  0.92 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 14400/23007 batches | lr 0.0001 | ms/batch 498.12 | loss  0.85 | cls  0.85 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 14500/23007 batches | lr 0.0001 | ms/batch 497.96 | loss  0.87 | cls  0.87 | err  0.21 | \n",
      "scGPT - INFO - | epoch   2 | 14600/23007 batches | lr 0.0001 | ms/batch 498.22 | loss  0.89 | cls  0.89 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 14700/23007 batches | lr 0.0001 | ms/batch 497.95 | loss  0.90 | cls  0.90 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 14800/23007 batches | lr 0.0001 | ms/batch 497.97 | loss  0.92 | cls  0.92 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 14900/23007 batches | lr 0.0001 | ms/batch 498.06 | loss  0.94 | cls  0.94 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 15000/23007 batches | lr 0.0001 | ms/batch 498.26 | loss  0.87 | cls  0.87 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 15100/23007 batches | lr 0.0001 | ms/batch 497.95 | loss  0.90 | cls  0.90 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 15200/23007 batches | lr 0.0001 | ms/batch 497.63 | loss  0.92 | cls  0.92 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 15300/23007 batches | lr 0.0001 | ms/batch 497.67 | loss  0.86 | cls  0.86 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 15400/23007 batches | lr 0.0001 | ms/batch 497.92 | loss  0.90 | cls  0.90 | err  0.21 | \n",
      "scGPT - INFO - | epoch   2 | 15500/23007 batches | lr 0.0001 | ms/batch 497.82 | loss  0.91 | cls  0.91 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 15600/23007 batches | lr 0.0001 | ms/batch 497.89 | loss  0.95 | cls  0.95 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 15700/23007 batches | lr 0.0001 | ms/batch 497.94 | loss  0.90 | cls  0.90 | err  0.21 | \n",
      "scGPT - INFO - | epoch   2 | 15800/23007 batches | lr 0.0001 | ms/batch 497.72 | loss  0.86 | cls  0.86 | err  0.21 | \n",
      "scGPT - INFO - | epoch   2 | 15900/23007 batches | lr 0.0001 | ms/batch 497.75 | loss  0.90 | cls  0.90 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 16000/23007 batches | lr 0.0001 | ms/batch 498.20 | loss  0.82 | cls  0.82 | err  0.18 | \n",
      "scGPT - INFO - | epoch   2 | 16100/23007 batches | lr 0.0001 | ms/batch 497.68 | loss  0.80 | cls  0.80 | err  0.19 | \n",
      "scGPT - INFO - | epoch   2 | 16200/23007 batches | lr 0.0001 | ms/batch 498.01 | loss  0.81 | cls  0.81 | err  0.20 | \n",
      "scGPT - INFO - | epoch   2 | 16300/23007 batches | lr 0.0001 | ms/batch 497.75 | loss  0.84 | cls  0.84 | err  0.21 | \n",
      "scGPT - INFO - | epoch   2 | 16400/23007 batches | lr 0.0001 | ms/batch 497.68 | loss  0.88 | cls  0.88 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 16500/23007 batches | lr 0.0001 | ms/batch 497.58 | loss  0.90 | cls  0.90 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 16600/23007 batches | lr 0.0001 | ms/batch 497.68 | loss  0.87 | cls  0.87 | err  0.21 | \n",
      "scGPT - INFO - | epoch   2 | 16700/23007 batches | lr 0.0001 | ms/batch 497.64 | loss  0.86 | cls  0.86 | err  0.21 | \n",
      "scGPT - INFO - | epoch   2 | 16800/23007 batches | lr 0.0001 | ms/batch 497.56 | loss  0.87 | cls  0.87 | err  0.21 | \n",
      "scGPT - INFO - | epoch   2 | 16900/23007 batches | lr 0.0001 | ms/batch 497.66 | loss  0.93 | cls  0.93 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 17000/23007 batches | lr 0.0001 | ms/batch 498.26 | loss  0.91 | cls  0.91 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 17100/23007 batches | lr 0.0001 | ms/batch 497.74 | loss  0.96 | cls  0.96 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 17200/23007 batches | lr 0.0001 | ms/batch 497.71 | loss  0.85 | cls  0.85 | err  0.21 | \n",
      "scGPT - INFO - | epoch   2 | 17300/23007 batches | lr 0.0001 | ms/batch 497.81 | loss  0.93 | cls  0.93 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 17400/23007 batches | lr 0.0001 | ms/batch 497.46 | loss  0.88 | cls  0.88 | err  0.20 | \n",
      "scGPT - INFO - | epoch   2 | 17500/23007 batches | lr 0.0001 | ms/batch 497.60 | loss  0.82 | cls  0.82 | err  0.20 | \n",
      "scGPT - INFO - | epoch   2 | 17600/23007 batches | lr 0.0001 | ms/batch 497.64 | loss  0.80 | cls  0.80 | err  0.20 | \n",
      "scGPT - INFO - | epoch   2 | 17700/23007 batches | lr 0.0001 | ms/batch 497.75 | loss  0.79 | cls  0.79 | err  0.20 | \n",
      "scGPT - INFO - | epoch   2 | 17800/23007 batches | lr 0.0001 | ms/batch 497.70 | loss  0.90 | cls  0.90 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 17900/23007 batches | lr 0.0001 | ms/batch 497.86 | loss  0.84 | cls  0.84 | err  0.20 | \n",
      "scGPT - INFO - | epoch   2 | 18000/23007 batches | lr 0.0001 | ms/batch 498.49 | loss  0.87 | cls  0.87 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 18100/23007 batches | lr 0.0001 | ms/batch 498.06 | loss  0.89 | cls  0.89 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 18200/23007 batches | lr 0.0001 | ms/batch 497.91 | loss  0.91 | cls  0.91 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 18300/23007 batches | lr 0.0001 | ms/batch 497.83 | loss  0.88 | cls  0.88 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 18400/23007 batches | lr 0.0001 | ms/batch 498.24 | loss  0.95 | cls  0.95 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 18500/23007 batches | lr 0.0001 | ms/batch 498.15 | loss  0.74 | cls  0.74 | err  0.18 | \n",
      "scGPT - INFO - | epoch   2 | 18600/23007 batches | lr 0.0001 | ms/batch 497.72 | loss  0.86 | cls  0.86 | err  0.21 | \n",
      "scGPT - INFO - | epoch   2 | 18700/23007 batches | lr 0.0001 | ms/batch 497.73 | loss  0.86 | cls  0.86 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 18800/23007 batches | lr 0.0001 | ms/batch 497.71 | loss  0.82 | cls  0.82 | err  0.20 | \n",
      "scGPT - INFO - | epoch   2 | 18900/23007 batches | lr 0.0001 | ms/batch 497.47 | loss  0.87 | cls  0.87 | err  0.21 | \n",
      "scGPT - INFO - | epoch   2 | 19000/23007 batches | lr 0.0001 | ms/batch 498.33 | loss  0.89 | cls  0.89 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 19100/23007 batches | lr 0.0001 | ms/batch 497.50 | loss  0.88 | cls  0.88 | err  0.21 | \n",
      "scGPT - INFO - | epoch   2 | 19200/23007 batches | lr 0.0001 | ms/batch 497.63 | loss  0.89 | cls  0.89 | err  0.20 | \n",
      "scGPT - INFO - | epoch   2 | 19300/23007 batches | lr 0.0001 | ms/batch 497.80 | loss  0.87 | cls  0.87 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 19400/23007 batches | lr 0.0001 | ms/batch 498.12 | loss  0.88 | cls  0.88 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 19500/23007 batches | lr 0.0001 | ms/batch 497.96 | loss  0.85 | cls  0.85 | err  0.21 | \n",
      "scGPT - INFO - | epoch   2 | 19600/23007 batches | lr 0.0001 | ms/batch 497.95 | loss  0.78 | cls  0.78 | err  0.19 | \n",
      "scGPT - INFO - | epoch   2 | 19700/23007 batches | lr 0.0001 | ms/batch 497.62 | loss  0.86 | cls  0.86 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 19800/23007 batches | lr 0.0001 | ms/batch 497.77 | loss  0.89 | cls  0.89 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 19900/23007 batches | lr 0.0001 | ms/batch 497.89 | loss  0.87 | cls  0.87 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 20000/23007 batches | lr 0.0001 | ms/batch 498.58 | loss  0.92 | cls  0.92 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 20100/23007 batches | lr 0.0001 | ms/batch 497.62 | loss  0.87 | cls  0.87 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 20200/23007 batches | lr 0.0001 | ms/batch 497.59 | loss  0.94 | cls  0.94 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 20300/23007 batches | lr 0.0001 | ms/batch 497.81 | loss  0.91 | cls  0.91 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 20400/23007 batches | lr 0.0001 | ms/batch 497.68 | loss  0.92 | cls  0.92 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 20500/23007 batches | lr 0.0001 | ms/batch 498.02 | loss  0.88 | cls  0.88 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 20600/23007 batches | lr 0.0001 | ms/batch 498.08 | loss  0.86 | cls  0.86 | err  0.21 | \n",
      "scGPT - INFO - | epoch   2 | 20700/23007 batches | lr 0.0001 | ms/batch 497.79 | loss  0.84 | cls  0.84 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 20800/23007 batches | lr 0.0001 | ms/batch 498.15 | loss  0.84 | cls  0.84 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 20900/23007 batches | lr 0.0001 | ms/batch 497.87 | loss  0.89 | cls  0.89 | err  0.23 | \n",
      "scGPT - INFO - | epoch   2 | 21000/23007 batches | lr 0.0001 | ms/batch 498.11 | loss  0.85 | cls  0.85 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 21100/23007 batches | lr 0.0001 | ms/batch 498.19 | loss  0.79 | cls  0.79 | err  0.19 | \n",
      "scGPT - INFO - | epoch   2 | 21200/23007 batches | lr 0.0001 | ms/batch 497.92 | loss  0.87 | cls  0.87 | err  0.20 | \n",
      "scGPT - INFO - | epoch   2 | 21300/23007 batches | lr 0.0001 | ms/batch 497.73 | loss  0.82 | cls  0.82 | err  0.21 | \n",
      "scGPT - INFO - | epoch   2 | 21400/23007 batches | lr 0.0001 | ms/batch 497.96 | loss  0.83 | cls  0.83 | err  0.21 | \n",
      "scGPT - INFO - | epoch   2 | 21500/23007 batches | lr 0.0001 | ms/batch 497.79 | loss  0.88 | cls  0.88 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 21600/23007 batches | lr 0.0001 | ms/batch 498.20 | loss  0.84 | cls  0.84 | err  0.20 | \n",
      "scGPT - INFO - | epoch   2 | 21700/23007 batches | lr 0.0001 | ms/batch 498.08 | loss  0.82 | cls  0.82 | err  0.19 | \n",
      "scGPT - INFO - | epoch   2 | 21800/23007 batches | lr 0.0001 | ms/batch 498.08 | loss  0.83 | cls  0.83 | err  0.21 | \n",
      "scGPT - INFO - | epoch   2 | 21900/23007 batches | lr 0.0001 | ms/batch 498.34 | loss  0.87 | cls  0.87 | err  0.21 | \n",
      "scGPT - INFO - | epoch   2 | 22000/23007 batches | lr 0.0001 | ms/batch 498.57 | loss  0.78 | cls  0.78 | err  0.20 | \n",
      "scGPT - INFO - | epoch   2 | 22100/23007 batches | lr 0.0001 | ms/batch 497.91 | loss  0.89 | cls  0.89 | err  0.22 | \n",
      "scGPT - INFO - | epoch   2 | 22200/23007 batches | lr 0.0001 | ms/batch 498.04 | loss  0.94 | cls  0.94 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 22300/23007 batches | lr 0.0001 | ms/batch 497.64 | loss  0.78 | cls  0.78 | err  0.20 | \n",
      "scGPT - INFO - | epoch   2 | 22400/23007 batches | lr 0.0001 | ms/batch 497.85 | loss  0.83 | cls  0.83 | err  0.20 | \n",
      "scGPT - INFO - | epoch   2 | 22500/23007 batches | lr 0.0001 | ms/batch 497.80 | loss  0.84 | cls  0.84 | err  0.19 | \n",
      "scGPT - INFO - | epoch   2 | 22600/23007 batches | lr 0.0001 | ms/batch 497.66 | loss  0.87 | cls  0.87 | err  0.21 | \n",
      "scGPT - INFO - | epoch   2 | 22700/23007 batches | lr 0.0001 | ms/batch 497.74 | loss  0.82 | cls  0.82 | err  0.19 | \n",
      "scGPT - INFO - | epoch   2 | 22800/23007 batches | lr 0.0001 | ms/batch 497.69 | loss  0.75 | cls  0.75 | err  0.18 | \n",
      "scGPT - INFO - | epoch   2 | 22900/23007 batches | lr 0.0001 | ms/batch 498.01 | loss  0.88 | cls  0.88 | err  0.24 | \n",
      "scGPT - INFO - | epoch   2 | 23000/23007 batches | lr 0.0001 | ms/batch 498.16 | loss  0.83 | cls  0.83 | err  0.21 | \n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - | end of epoch   2 | time: 11962.68s | valid loss/mse 0.9309 | err 0.2369\n",
      "scGPT - INFO - -----------------------------------------------------------------------------------------\n",
      "scGPT - INFO - Best model with score 0.9309\n",
      "random masking at epoch   3, ratio of masked values in train:  0.0000\n",
      "scGPT - INFO - | epoch   3 | 100/23007 batches | lr 0.0001 | ms/batch 514.06 | loss  0.79 | cls  0.79 | err  0.20 | \n",
      "scGPT - INFO - | epoch   3 | 200/23007 batches | lr 0.0001 | ms/batch 499.70 | loss  0.78 | cls  0.78 | err  0.22 | \n",
      "scGPT - INFO - | epoch   3 | 300/23007 batches | lr 0.0001 | ms/batch 498.39 | loss  0.76 | cls  0.76 | err  0.18 | \n",
      "scGPT - INFO - | epoch   3 | 400/23007 batches | lr 0.0001 | ms/batch 498.22 | loss  0.85 | cls  0.85 | err  0.21 | \n",
      "scGPT - INFO - | epoch   3 | 500/23007 batches | lr 0.0001 | ms/batch 498.27 | loss  0.77 | cls  0.77 | err  0.18 | \n",
      "scGPT - INFO - | epoch   3 | 600/23007 batches | lr 0.0001 | ms/batch 498.32 | loss  0.78 | cls  0.78 | err  0.20 | \n",
      "scGPT - INFO - | epoch   3 | 700/23007 batches | lr 0.0001 | ms/batch 498.25 | loss  0.77 | cls  0.77 | err  0.20 | \n",
      "scGPT - INFO - | epoch   3 | 800/23007 batches | lr 0.0001 | ms/batch 498.10 | loss  0.82 | cls  0.82 | err  0.21 | \n",
      "scGPT - INFO - | epoch   3 | 900/23007 batches | lr 0.0001 | ms/batch 498.79 | loss  0.77 | cls  0.77 | err  0.19 | \n",
      "scGPT - INFO - | epoch   3 | 1000/23007 batches | lr 0.0001 | ms/batch 498.28 | loss  0.72 | cls  0.72 | err  0.17 | \n",
      "scGPT - INFO - | epoch   3 | 1100/23007 batches | lr 0.0001 | ms/batch 498.08 | loss  0.82 | cls  0.82 | err  0.20 | \n",
      "scGPT - INFO - | epoch   3 | 1200/23007 batches | lr 0.0001 | ms/batch 497.93 | loss  0.93 | cls  0.93 | err  0.23 | \n",
      "scGPT - INFO - | epoch   3 | 1300/23007 batches | lr 0.0001 | ms/batch 497.79 | loss  0.79 | cls  0.79 | err  0.19 | \n",
      "scGPT - INFO - | epoch   3 | 1400/23007 batches | lr 0.0001 | ms/batch 497.38 | loss  0.78 | cls  0.78 | err  0.20 | \n",
      "scGPT - INFO - | epoch   3 | 1500/23007 batches | lr 0.0001 | ms/batch 497.77 | loss  0.82 | cls  0.82 | err  0.20 | \n",
      "scGPT - INFO - | epoch   3 | 1600/23007 batches | lr 0.0001 | ms/batch 497.60 | loss  0.82 | cls  0.82 | err  0.20 | \n",
      "scGPT - INFO - | epoch   3 | 1700/23007 batches | lr 0.0001 | ms/batch 497.39 | loss  0.82 | cls  0.82 | err  0.20 | \n",
      "scGPT - INFO - | epoch   3 | 1800/23007 batches | lr 0.0001 | ms/batch 497.76 | loss  0.83 | cls  0.83 | err  0.21 | \n",
      "scGPT - INFO - | epoch   3 | 1900/23007 batches | lr 0.0001 | ms/batch 497.64 | loss  0.82 | cls  0.82 | err  0.20 | \n",
      "scGPT - INFO - | epoch   3 | 2000/23007 batches | lr 0.0001 | ms/batch 498.32 | loss  0.77 | cls  0.77 | err  0.19 | \n",
      "scGPT - INFO - | epoch   3 | 2100/23007 batches | lr 0.0001 | ms/batch 497.95 | loss  0.80 | cls  0.80 | err  0.18 | \n",
      "scGPT - INFO - | epoch   3 | 2200/23007 batches | lr 0.0001 | ms/batch 497.61 | loss  0.79 | cls  0.79 | err  0.21 | \n",
      "scGPT - INFO - | epoch   3 | 2300/23007 batches | lr 0.0001 | ms/batch 497.89 | loss  0.86 | cls  0.86 | err  0.20 | \n",
      "scGPT - INFO - | epoch   3 | 2400/23007 batches | lr 0.0001 | ms/batch 498.02 | loss  0.72 | cls  0.72 | err  0.18 | \n",
      "scGPT - INFO - | epoch   3 | 2500/23007 batches | lr 0.0001 | ms/batch 497.81 | loss  0.76 | cls  0.76 | err  0.18 | \n",
      "scGPT - INFO - | epoch   3 | 2600/23007 batches | lr 0.0001 | ms/batch 497.96 | loss  0.89 | cls  0.89 | err  0.23 | \n",
      "scGPT - INFO - | epoch   3 | 2700/23007 batches | lr 0.0001 | ms/batch 497.78 | loss  0.78 | cls  0.78 | err  0.19 | \n",
      "scGPT - INFO - | epoch   3 | 2800/23007 batches | lr 0.0001 | ms/batch 497.72 | loss  0.82 | cls  0.82 | err  0.21 | \n",
      "scGPT - INFO - | epoch   3 | 2900/23007 batches | lr 0.0001 | ms/batch 497.52 | loss  0.80 | cls  0.80 | err  0.20 | \n",
      "scGPT - INFO - | epoch   3 | 3000/23007 batches | lr 0.0001 | ms/batch 498.24 | loss  0.79 | cls  0.79 | err  0.20 | \n",
      "scGPT - INFO - | epoch   3 | 3100/23007 batches | lr 0.0001 | ms/batch 498.63 | loss  0.81 | cls  0.81 | err  0.19 | \n",
      "scGPT - INFO - | epoch   3 | 3200/23007 batches | lr 0.0001 | ms/batch 498.65 | loss  0.81 | cls  0.81 | err  0.22 | \n",
      "scGPT - INFO - | epoch   3 | 3300/23007 batches | lr 0.0001 | ms/batch 497.95 | loss  0.80 | cls  0.80 | err  0.20 | \n",
      "scGPT - INFO - | epoch   3 | 3400/23007 batches | lr 0.0001 | ms/batch 497.96 | loss  0.82 | cls  0.82 | err  0.21 | \n",
      "scGPT - INFO - | epoch   3 | 3500/23007 batches | lr 0.0001 | ms/batch 499.29 | loss  0.80 | cls  0.80 | err  0.20 | \n",
      "scGPT - INFO - | epoch   3 | 3600/23007 batches | lr 0.0001 | ms/batch 497.72 | loss  0.75 | cls  0.75 | err  0.19 | \n",
      "scGPT - INFO - | epoch   3 | 3700/23007 batches | lr 0.0001 | ms/batch 497.70 | loss  0.84 | cls  0.84 | err  0.21 | \n",
      "scGPT - INFO - | epoch   3 | 3800/23007 batches | lr 0.0001 | ms/batch 497.64 | loss  0.80 | cls  0.80 | err  0.18 | \n",
      "scGPT - INFO - | epoch   3 | 3900/23007 batches | lr 0.0001 | ms/batch 497.58 | loss  0.79 | cls  0.79 | err  0.20 | \n",
      "scGPT - INFO - | epoch   3 | 4000/23007 batches | lr 0.0001 | ms/batch 498.28 | loss  0.76 | cls  0.76 | err  0.19 | \n",
      "scGPT - INFO - | epoch   3 | 4100/23007 batches | lr 0.0001 | ms/batch 498.00 | loss  0.77 | cls  0.77 | err  0.19 | \n",
      "scGPT - INFO - | epoch   3 | 4200/23007 batches | lr 0.0001 | ms/batch 497.65 | loss  0.85 | cls  0.85 | err  0.22 | \n",
      "scGPT - INFO - | epoch   3 | 4300/23007 batches | lr 0.0001 | ms/batch 497.59 | loss  0.77 | cls  0.77 | err  0.20 | \n",
      "scGPT - INFO - | epoch   3 | 4400/23007 batches | lr 0.0001 | ms/batch 497.68 | loss  0.79 | cls  0.79 | err  0.19 | \n",
      "scGPT - INFO - | epoch   3 | 4500/23007 batches | lr 0.0001 | ms/batch 497.57 | loss  0.82 | cls  0.82 | err  0.20 | \n",
      "scGPT - INFO - | epoch   3 | 4600/23007 batches | lr 0.0001 | ms/batch 497.85 | loss  0.80 | cls  0.80 | err  0.20 | \n",
      "scGPT - INFO - | epoch   3 | 4700/23007 batches | lr 0.0001 | ms/batch 497.83 | loss  0.82 | cls  0.82 | err  0.20 | \n",
      "scGPT - INFO - | epoch   3 | 4800/23007 batches | lr 0.0001 | ms/batch 497.85 | loss  0.84 | cls  0.84 | err  0.21 | \n",
      "scGPT - INFO - | epoch   3 | 4900/23007 batches | lr 0.0001 | ms/batch 497.69 | loss  0.81 | cls  0.81 | err  0.20 | \n",
      "scGPT - INFO - | epoch   3 | 5000/23007 batches | lr 0.0001 | ms/batch 498.20 | loss  0.89 | cls  0.89 | err  0.21 | \n",
      "scGPT - INFO - | epoch   3 | 5100/23007 batches | lr 0.0001 | ms/batch 498.07 | loss  0.75 | cls  0.75 | err  0.19 | \n",
      "scGPT - INFO - | epoch   3 | 5200/23007 batches | lr 0.0001 | ms/batch 497.71 | loss  0.83 | cls  0.83 | err  0.20 | \n",
      "scGPT - INFO - | epoch   3 | 5300/23007 batches | lr 0.0001 | ms/batch 498.13 | loss  0.78 | cls  0.78 | err  0.19 | \n",
      "scGPT - INFO - | epoch   3 | 5400/23007 batches | lr 0.0001 | ms/batch 498.11 | loss  0.77 | cls  0.77 | err  0.18 | \n",
      "scGPT - INFO - | epoch   3 | 5500/23007 batches | lr 0.0001 | ms/batch 497.81 | loss  0.76 | cls  0.76 | err  0.20 | \n",
      "scGPT - INFO - | epoch   3 | 5600/23007 batches | lr 0.0001 | ms/batch 497.67 | loss  0.80 | cls  0.80 | err  0.18 | \n",
      "scGPT - INFO - | epoch   3 | 5700/23007 batches | lr 0.0001 | ms/batch 497.47 | loss  0.77 | cls  0.77 | err  0.20 | \n",
      "scGPT - INFO - | epoch   3 | 5800/23007 batches | lr 0.0001 | ms/batch 497.70 | loss  0.85 | cls  0.85 | err  0.21 | \n",
      "scGPT - INFO - | epoch   3 | 5900/23007 batches | lr 0.0001 | ms/batch 497.67 | loss  0.74 | cls  0.74 | err  0.18 | \n",
      "scGPT - INFO - | epoch   3 | 6000/23007 batches | lr 0.0001 | ms/batch 498.11 | loss  0.79 | cls  0.79 | err  0.20 | \n",
      "scGPT - INFO - | epoch   3 | 6100/23007 batches | lr 0.0001 | ms/batch 497.49 | loss  0.79 | cls  0.79 | err  0.21 | \n",
      "scGPT - INFO - | epoch   3 | 6200/23007 batches | lr 0.0001 | ms/batch 497.57 | loss  0.85 | cls  0.85 | err  0.20 | \n",
      "scGPT - INFO - | epoch   3 | 6300/23007 batches | lr 0.0001 | ms/batch 497.70 | loss  0.81 | cls  0.81 | err  0.20 | \n",
      "scGPT - INFO - | epoch   3 | 6400/23007 batches | lr 0.0001 | ms/batch 498.16 | loss  0.76 | cls  0.76 | err  0.19 | \n",
      "scGPT - INFO - | epoch   3 | 6500/23007 batches | lr 0.0001 | ms/batch 497.73 | loss  0.73 | cls  0.73 | err  0.17 | \n",
      "scGPT - INFO - | epoch   3 | 6600/23007 batches | lr 0.0001 | ms/batch 497.64 | loss  0.82 | cls  0.82 | err  0.22 | \n",
      "scGPT - INFO - | epoch   3 | 6700/23007 batches | lr 0.0001 | ms/batch 497.87 | loss  0.84 | cls  0.84 | err  0.23 | \n",
      "scGPT - INFO - | epoch   3 | 6800/23007 batches | lr 0.0001 | ms/batch 497.67 | loss  0.85 | cls  0.85 | err  0.20 | \n",
      "scGPT - INFO - | epoch   3 | 6900/23007 batches | lr 0.0001 | ms/batch 497.84 | loss  0.77 | cls  0.77 | err  0.19 | \n",
      "scGPT - INFO - | epoch   3 | 7000/23007 batches | lr 0.0001 | ms/batch 498.11 | loss  0.74 | cls  0.74 | err  0.18 | \n",
      "scGPT - INFO - | epoch   3 | 7100/23007 batches | lr 0.0001 | ms/batch 497.70 | loss  0.88 | cls  0.88 | err  0.23 | \n",
      "scGPT - INFO - | epoch   3 | 7200/23007 batches | lr 0.0001 | ms/batch 497.83 | loss  0.81 | cls  0.81 | err  0.19 | \n",
      "scGPT - INFO - | epoch   3 | 7300/23007 batches | lr 0.0001 | ms/batch 498.29 | loss  0.78 | cls  0.78 | err  0.20 | \n",
      "scGPT - INFO - | epoch   3 | 7400/23007 batches | lr 0.0001 | ms/batch 497.83 | loss  0.74 | cls  0.74 | err  0.18 | \n",
      "scGPT - INFO - | epoch   3 | 7500/23007 batches | lr 0.0001 | ms/batch 504.06 | loss  0.85 | cls  0.85 | err  0.20 | \n",
      "scGPT - INFO - | epoch   3 | 7600/23007 batches | lr 0.0001 | ms/batch 503.00 | loss  0.77 | cls  0.77 | err  0.20 | \n",
      "scGPT - INFO - | epoch   3 | 7700/23007 batches | lr 0.0001 | ms/batch 502.17 | loss  0.74 | cls  0.74 | err  0.18 | \n",
      "scGPT - INFO - | epoch   3 | 7800/23007 batches | lr 0.0001 | ms/batch 503.62 | loss  0.86 | cls  0.86 | err  0.22 | \n",
      "scGPT - INFO - | epoch   3 | 7900/23007 batches | lr 0.0001 | ms/batch 501.27 | loss  0.71 | cls  0.71 | err  0.18 | \n",
      "scGPT - INFO - | epoch   3 | 8000/23007 batches | lr 0.0001 | ms/batch 502.94 | loss  0.72 | cls  0.72 | err  0.17 | \n",
      "scGPT - INFO - | epoch   3 | 8100/23007 batches | lr 0.0001 | ms/batch 501.68 | loss  0.83 | cls  0.83 | err  0.21 | \n",
      "scGPT - INFO - | epoch   3 | 8200/23007 batches | lr 0.0001 | ms/batch 501.43 | loss  0.86 | cls  0.86 | err  0.22 | \n",
      "scGPT - INFO - | epoch   3 | 8300/23007 batches | lr 0.0001 | ms/batch 501.74 | loss  0.73 | cls  0.73 | err  0.18 | \n",
      "scGPT - INFO - | epoch   3 | 8400/23007 batches | lr 0.0001 | ms/batch 501.18 | loss  0.71 | cls  0.71 | err  0.18 | \n"
     ]
    }
   ],
   "source": [
    "# Early stopping parameters\n",
    "patience = 2  # Number of epochs to wait for improvement\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model = None\n",
    "best_model_epoch = 0\n",
    "\n",
    "define_wandb_metrics()\n",
    "\n",
    "for epoch in range(1, epochs + 1):  # epochs is the maximum number of epochs\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Prepare data for this epoch\n",
    "    train_data_pt, valid_data_pt = prepare_data(sort_seq_batch=per_seq_batch_sample)\n",
    "    train_loader = prepare_dataloader(\n",
    "        train_data_pt,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        intra_domain_shuffle=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    valid_loader = prepare_dataloader(\n",
    "        valid_data_pt,\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        intra_domain_shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    \n",
    "    # Train for one epoch\n",
    "    if config.do_train:\n",
    "        train(model, loader=train_loader)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_loss, val_err = evaluate(model, loader=valid_loader)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    logger.info(\"-\" * 89)\n",
    "    logger.info(\n",
    "        f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "        f\"valid loss/mse {val_loss:5.4f} | err {val_err:5.4f}\"\n",
    "    )\n",
    "    logger.info(\"-\" * 89)\n",
    "    \n",
    "    # Check for improvement\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "        best_model_epoch = epoch\n",
    "        epochs_without_improvement = 0  # Reset counter\n",
    "        logger.info(f\"Best model with score {best_val_loss:5.4f}\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "    \n",
    "    # Stop training if no improvement for `patience` epochs\n",
    "    if epochs_without_improvement >= patience:\n",
    "        logger.info(f\"Early stopping triggered after {epoch} epochs.\")\n",
    "        break\n",
    "    \n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step()\n",
    "    if DAB_separate_optim:\n",
    "        scheduler_dab.step()\n",
    "    if ADV:\n",
    "        scheduler_D.step()\n",
    "        scheduler_E.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a36630c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model.state_dict(), save_dir / \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6ce176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% inference\n",
    "def test(model: nn.Module, adata: DataLoader) -> float:\n",
    "    all_counts = (\n",
    "        adata.layers[input_layer_key].toarray()\n",
    "        if issparse(adata.layers[input_layer_key])\n",
    "        else adata.layers[input_layer_key]\n",
    "    )\n",
    "\n",
    "    celltypes_labels = adata.obs[\"celltype_id\"].tolist()  # make sure count from 0\n",
    "    celltypes_labels = np.array(celltypes_labels)\n",
    "\n",
    "    batch_ids = adata.obs[\"batch_id\"].tolist()\n",
    "    batch_ids = np.array(batch_ids)\n",
    "\n",
    "    tokenized_test = tokenize_and_pad_batch(\n",
    "        all_counts,\n",
    "        gene_ids,\n",
    "        max_len=max_seq_len,\n",
    "        vocab=vocab,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        append_cls=True,  # append <cls> token at the beginning\n",
    "        include_zero_gene=include_zero_gene,\n",
    "    )\n",
    "\n",
    "    input_values_test = random_mask_value(\n",
    "        tokenized_test[\"values\"],\n",
    "        mask_ratio=mask_ratio,\n",
    "        mask_value=mask_value,\n",
    "        pad_value=pad_value,\n",
    "    )\n",
    "\n",
    "    test_data_pt = {\n",
    "        \"gene_ids\": tokenized_test[\"genes\"],\n",
    "        \"values\": input_values_test,\n",
    "        \"target_values\": tokenized_test[\"values\"],\n",
    "        \"batch_labels\": torch.from_numpy(batch_ids).long(),\n",
    "        \"celltype_labels\": torch.from_numpy(celltypes_labels).long(),\n",
    "    }\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=SeqDataset(test_data_pt),\n",
    "        batch_size=eval_batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=min(len(os.sched_getaffinity(0)), eval_batch_size // 2),\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    predictions = evaluate(\n",
    "        model,\n",
    "        loader=test_loader,\n",
    "        return_raw=True,\n",
    "    )\n",
    "\n",
    "    # compute accuracy, precision, recall, f1\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "    accuracy = accuracy_score(celltypes_labels, predictions)\n",
    "    precision = precision_score(celltypes_labels, predictions, average=\"macro\")\n",
    "    recall = recall_score(celltypes_labels, predictions, average=\"macro\")\n",
    "    macro_f1 = f1_score(celltypes_labels, predictions, average=\"macro\")\n",
    "\n",
    "    logger.info(\n",
    "        f\"Accuracy: {accuracy:.3f}, Precision: {precision:.3f}, Recall: {recall:.3f}, \"\n",
    "        f\"Macro F1: {macro_f1:.3f}\"\n",
    "    )\n",
    "\n",
    "    results = {\n",
    "        \"test/accuracy\": accuracy,\n",
    "        \"test/precision\": precision,\n",
    "        \"test/recall\": recall,\n",
    "        \"test/macro_f1\": macro_f1,\n",
    "    }\n",
    "\n",
    "    return predictions, celltypes_labels, results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16236bf2",
   "metadata": {},
   "source": [
    "## Step 5: Inference with fine-tuned scGPT model\n",
    "In the cell-type annotation task, the fine-tuned scGPT predicts cell-type labels for query set as inference. The model performance is evaluated on standard classificaton metrics. Here we visualize the predicted labels over the scGPT cell embeddings, and present the confusion matrix for detailed classification performance on the cell-group level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79730e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels, results = test(best_model, adata_test)\n",
    "adata_test_raw.obs[\"predictions\"] = [id2type[p] for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37db3df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute neighbors (required for UMAP)\n",
    "sc.pp.neighbors(adata_test_raw, n_neighbors=15, use_rep=\"X\")\n",
    "\n",
    "# Compute UMAP\n",
    "sc.tl.umap(adata_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb366954",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_test_raw.write_h5ad(\"../data/datasets/czi_covid_pbmc_predictions_umap_peft.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7f63ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Simplified hyperparameter setup\n",
    "# hyperparameter_defaults = dict(\n",
    "#     seed=0,\n",
    "#     dataset_name=\"ms\",\n",
    "#     load_model=\"../models/scGPT_human\",  # Path to pre-trained model\n",
    "#     epochs=10,\n",
    "#     lr=1e-4,\n",
    "#     batch_size=32,\n",
    "#     layer_size=128,\n",
    "#     nlayers=4,\n",
    "#     nhead=4,\n",
    "#     dropout=0.2,\n",
    "#     max_seq_len=3001,\n",
    "#     n_bins=51,\n",
    "#     CLS=True,  # Enable cell type classification\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d9e5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Simplified data loading and preprocessing\n",
    "# data_dir = Path(\"../data/sample_ms\")\n",
    "# model_dir = Path(\"../models/scgpt_human\")\n",
    "\n",
    "# adata = sc.read(data_dir / \"c_data.h5ad\")\n",
    "# adata_test = sc.read(data_dir / \"filtered_ms_adata.h5ad\")\n",
    "\n",
    "# # Set cell type as the target\n",
    "# adata.obs[\"celltype\"] = adata.obs[\"Factor Value[inferred cell type - authors labels]\"].astype(\"category\")\n",
    "# adata_test.obs[\"celltype\"] = adata_test.obs[\"Factor Value[inferred cell type - authors labels]\"].astype(\"category\")\n",
    "\n",
    "# # Preprocess data\n",
    "# preprocessor = Preprocessor(\n",
    "#     use_key=\"X\",\n",
    "#     normalize_total=1e4,\n",
    "#     result_normed_key=\"X_normed\",\n",
    "#     binning=hyperparameter_defaults[\"n_bins\"],\n",
    "#     result_binned_key=\"X_binned\",\n",
    "# )\n",
    "# preprocessor(adata)\n",
    "# preprocessor(adata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b04c850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Simplified tokenization\n",
    "# input_emb_style = \"continuous\"  # \"category\" or \"continuous\" or \"scaling\"\n",
    "# pad_token = \"<pad>\"\n",
    "# if input_emb_style == \"category\":\n",
    "#     pad_value = n_bins  # for padding gene expression values\n",
    "# else:\n",
    "#     pad_value = -2\n",
    "\n",
    "# vocab_file = Path(model_dir/\"vocab.json\")\n",
    "# vocab = GeneVocab.from_file(vocab_file)\n",
    "# gene_ids = np.array(vocab(adata.var.index.tolist()), dtype=int)\n",
    "\n",
    "# tokenized_train = tokenize_and_pad_batch(\n",
    "#     adata.layers[\"X_binned\"], \n",
    "#     gene_ids, \n",
    "#     max_len=hyperparameter_defaults[\"max_seq_len\"], \n",
    "#     vocab=vocab, \n",
    "#     append_cls=True,\n",
    "#     pad_token=pad_token, \n",
    "#     pad_value=pad_value\n",
    "# )\n",
    "# tokenized_valid = tokenize_and_pad_batch(\n",
    "#     adata_test.layers[\"X_binned\"], \n",
    "#     gene_ids, \n",
    "#     max_len=hyperparameter_defaults[\"max_seq_len\"], \n",
    "#     vocab=vocab, \n",
    "#     append_cls=True,\n",
    "#     pad_token=pad_token, \n",
    "#     pad_value=pad_value\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce5387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Simplified dataset and dataloader\n",
    "# class SeqDataset(Dataset):\n",
    "#     def __init__(self, data, labels):\n",
    "#         self.data = data\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.data[idx], self.labels[idx]\n",
    "\n",
    "# train_data, valid_data, train_labels, valid_labels = train_test_split(\n",
    "#     tokenized_train[\"genes\"], adata.obs[\"celltype\"].cat.codes, test_size=0.1, random_state=42\n",
    "# )\n",
    "\n",
    "# train_dataset = SeqDataset(train_data, train_labels)\n",
    "# valid_dataset = SeqDataset(valid_data, valid_labels)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=hyperparameter_defaults[\"batch_size\"], shuffle=True)\n",
    "# valid_loader = DataLoader(valid_dataset, batch_size=hyperparameter_defaults[\"batch_size\"], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64ac8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Simplified model definition\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# ntokens=len(vocab)\n",
    "# embsize=hyperparameter_defaults[\"layer_size\"]\n",
    "# model = TransformerModel(\n",
    "#     ntokens,\n",
    "#     embsize,\n",
    "#     nhead=hyperparameter_defaults[\"nhead\"],\n",
    "#     d_hid=hyperparameter_defaults[\"layer_size\"],\n",
    "#     nlayers=hyperparameter_defaults[\"nlayers\"],\n",
    "#     n_cls=len(adata.obs[\"celltype\"].cat.categories),\n",
    "#     vocab=vocab,\n",
    "#     dropout=hyperparameter_defaults[\"dropout\"],\n",
    "# ).to(device)\n",
    "\n",
    "# # Load pre-trained weights\n",
    "# if hyperparameter_defaults[\"load_model\"]:\n",
    "#     model.load_state_dict(torch.load(Path(hyperparameter_defaults[\"load_model\"]) / \"best_model.pt\"))\n",
    "#     print(\"Loaded pre-trained model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc89dd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Simplified evaluation\n",
    "# model.eval()\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# with torch.no_grad():\n",
    "#     for batch_data, batch_labels in valid_loader:\n",
    "#         batch_data, batch_labels = batch_data.to(device), batch_labels.to(device)\n",
    "#         output = model(batch_data)[\"cls_output\"]\n",
    "#         predictions = output.argmax(dim=1)\n",
    "#         correct += (predictions == batch_labels).sum().item()\n",
    "#         total += batch_labels.size(0)\n",
    "\n",
    "# print(f\"Validation Accuracy: {correct / total:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
