{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Efficient Finetuning (PEFT) using Low Rank Adapters (LoRA) for cell\n",
    "\n",
    "## Goals\n",
    "1. Write a PyTorch training loop implementing LoRA via PEFT\n",
    "2. Tokenize multiple sclerosis data first\n",
    "3. Parallelize LoRA on all GPUs if possible\n",
    "\n",
    "## Steps\n",
    "1. Integrate HuggingFace's PEFT into scGPT to perform finetuning\n",
    "2. Implementation will use HuggingFace's scGPT implementation from Therapeutic Commons - https://huggingface.co/tdc/scGPT\n",
    "3. Test dataset - M.S. dataset (since there is a benchmark)\n",
    "\n",
    "Requirements from HuggingFace\n",
    "- transformers \n",
    "- accelerate \n",
    "- evaluate\n",
    "- datasets \n",
    "- peft\n",
    "- loralib\n",
    "- PyTDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.50.3\n",
      "Accelerate version: 0.33.0\n",
      "Datasets version: 2.19.2\n"
     ]
    }
   ],
   "source": [
    "# HF imports\n",
    "import transformers\n",
    "import accelerate\n",
    "import datasets\n",
    "import torch\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "# TDC imports\n",
    "from tdc import tdc_hf_interface\n",
    "from tdc.model_server.tokenizers.scgpt import scGPTTokenizer\n",
    "\n",
    "# Version check (optional)\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Accelerate version: {accelerate.__version__}\")\n",
    "print(f\"Datasets version: {datasets.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load data\n",
    "\n",
    "1. Load raw counts from training and test dataset\n",
    "2. Follow steps for normalization, tokenization, and embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 3 GPUs!\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained scGPT model from Hugging Face\n",
    "scgpt = tdc_hf_interface(\"scGPT\")\n",
    "base_model = scgpt.load()\n",
    "tokenizer = scGPTTokenizer()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    base_model = nn.DataParallel(base_model)\n",
    "\n",
    "base_model = base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = \"../data/sample_ms/\"\n",
    "adata = sc.read_h5ad(data_path + \"c_data.h5ad\")\n",
    "adata_test = sc.read_h5ad(data_path + \"filtered_ms_adata.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n"
     ]
    }
   ],
   "source": [
    "gene_names = adata.var[\"gene_name\"].to_numpy()\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "adata.obs[\"cell_type_encoded\"] = le.fit_transform(adata.obs[\"celltype\"])\n",
    "\n",
    "# Tokenize train data\n",
    "train_tokens = tokenizer.tokenize_cell_vectors(adata.X.toarray(), gene_names)\n",
    "train_labels = adata.obs[\"cell_type_encoded\"].to_numpy()\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scRNADataset(Dataset):\n",
    "    def __init__(self, tokenized, labels=None):\n",
    "        self.tokenized = tokenized\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens, counts = self.tokenized[idx]\n",
    "        sample = {\n",
    "            \"input_ids\": torch.tensor(tokens, dtype=torch.long),\n",
    "            \"attention_mask\": (torch.tensor(counts) > 0).long(),\n",
    "            \"values\": torch.tensor(counts, dtype=torch.float),\n",
    "        }\n",
    "        if self.labels is not None:\n",
    "            sample[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return sample\n",
    "    \n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [b[\"input_ids\"] for b in batch]\n",
    "    attention_masks = [b[\"attention_mask\"] for b in batch]\n",
    "    values = [b[\"values\"] for b in batch]\n",
    "    labels = [b[\"labels\"] for b in batch] if \"labels\" in batch[0] else None\n",
    "\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=60694)  # <pad> token id\n",
    "    attention_masks_padded = pad_sequence(attention_masks, batch_first=True, padding_value=0).bool()\n",
    "    values_padded = pad_sequence(values, batch_first=True, padding_value=0.0)\n",
    "\n",
    "    result = {\n",
    "        \"input_ids\": input_ids_padded,\n",
    "        \"attention_mask\": attention_masks_padded,\n",
    "        \"values\": values_padded,\n",
    "    }\n",
    "\n",
    "    if labels is not None:\n",
    "        result[\"labels\"] = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Wrap base_model with classifier head\n",
    "class scGPTClassifier(nn.Module):\n",
    "    def __init__(self, base_model, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, values, labels=None):\n",
    "        outputs = self.base(input_ids=input_ids, attention_mask=attention_mask, values=values)\n",
    "\n",
    "        cls_token = outputs[\"cell_emb\"]  # use pooled cell embedding\n",
    "        logits = self.classifier(cls_token)\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels, weight=class_weights)\n",
    "        return {\"loss\": loss, \"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "num_classes = len(le.classes_)\n",
    "hidden_dim = 512  # common default for scGPT\n",
    "model = scGPTClassifier(base_model, hidden_dim, num_classes).to(device)\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_dataset = scRNADataset(train_tokens, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=96, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TRAINABLE: base.module.gene_encoder.embedding.weight\n",
      "✅ TRAINABLE: base.module.gene_encoder.enc_norm.weight\n",
      "✅ TRAINABLE: base.module.gene_encoder.enc_norm.bias\n",
      "✅ TRAINABLE: base.module.value_encoder.linear1.weight\n",
      "✅ TRAINABLE: base.module.value_encoder.linear1.bias\n",
      "✅ TRAINABLE: base.module.value_encoder.linear2.weight\n",
      "✅ TRAINABLE: base.module.value_encoder.linear2.bias\n",
      "✅ TRAINABLE: base.module.value_encoder.norm.weight\n",
      "✅ TRAINABLE: base.module.value_encoder.norm.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.0.self_attn.in_proj_weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.0.self_attn.in_proj_bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.0.self_attn.out_proj.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.0.self_attn.out_proj.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.0.linear1.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.0.linear1.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.0.linear2.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.0.linear2.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.0.norm1.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.0.norm1.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.0.norm2.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.0.norm2.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.1.self_attn.in_proj_weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.1.self_attn.in_proj_bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.1.self_attn.out_proj.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.1.self_attn.out_proj.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.1.linear1.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.1.linear1.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.1.linear2.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.1.linear2.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.1.norm1.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.1.norm1.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.1.norm2.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.1.norm2.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.2.self_attn.in_proj_weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.2.self_attn.in_proj_bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.2.self_attn.out_proj.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.2.self_attn.out_proj.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.2.linear1.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.2.linear1.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.2.linear2.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.2.linear2.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.2.norm1.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.2.norm1.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.2.norm2.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.2.norm2.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.3.self_attn.in_proj_weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.3.self_attn.in_proj_bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.3.self_attn.out_proj.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.3.self_attn.out_proj.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.3.linear1.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.3.linear1.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.3.linear2.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.3.linear2.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.3.norm1.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.3.norm1.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.3.norm2.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.3.norm2.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.4.self_attn.in_proj_weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.4.self_attn.in_proj_bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.4.self_attn.out_proj.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.4.self_attn.out_proj.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.4.linear1.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.4.linear1.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.4.linear2.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.4.linear2.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.4.norm1.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.4.norm1.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.4.norm2.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.4.norm2.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.5.self_attn.in_proj_weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.5.self_attn.in_proj_bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.5.self_attn.out_proj.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.5.self_attn.out_proj.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.5.linear1.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.5.linear1.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.5.linear2.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.5.linear2.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.5.norm1.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.5.norm1.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.5.norm2.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.5.norm2.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.6.self_attn.in_proj_weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.6.self_attn.in_proj_bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.6.self_attn.out_proj.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.6.self_attn.out_proj.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.6.linear1.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.6.linear1.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.6.linear2.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.6.linear2.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.6.norm1.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.6.norm1.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.6.norm2.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.6.norm2.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.7.self_attn.in_proj_weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.7.self_attn.in_proj_bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.7.self_attn.out_proj.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.7.self_attn.out_proj.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.7.linear1.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.7.linear1.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.7.linear2.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.7.linear2.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.7.norm1.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.7.norm1.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.7.norm2.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.7.norm2.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.8.self_attn.in_proj_weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.8.self_attn.in_proj_bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.8.self_attn.out_proj.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.8.self_attn.out_proj.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.8.linear1.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.8.linear1.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.8.linear2.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.8.linear2.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.8.norm1.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.8.norm1.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.8.norm2.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.8.norm2.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.9.self_attn.in_proj_weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.9.self_attn.in_proj_bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.9.self_attn.out_proj.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.9.self_attn.out_proj.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.9.linear1.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.9.linear1.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.9.linear2.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.9.linear2.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.9.norm1.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.9.norm1.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.9.norm2.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.9.norm2.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.10.self_attn.in_proj_weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.10.self_attn.in_proj_bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.10.self_attn.out_proj.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.10.self_attn.out_proj.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.10.linear1.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.10.linear1.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.10.linear2.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.10.linear2.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.10.norm1.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.10.norm1.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.10.norm2.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.10.norm2.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.11.self_attn.in_proj_weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.11.self_attn.in_proj_bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.11.self_attn.out_proj.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.11.self_attn.out_proj.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.11.linear1.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.11.linear1.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.11.linear2.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.11.linear2.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.11.norm1.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.11.norm1.bias\n",
      "✅ TRAINABLE: base.module.transformer.layers.11.norm2.weight\n",
      "✅ TRAINABLE: base.module.transformer.layers.11.norm2.bias\n",
      "✅ TRAINABLE: base.module.expr_decoder.fc.0.weight\n",
      "✅ TRAINABLE: base.module.expr_decoder.fc.0.bias\n",
      "✅ TRAINABLE: base.module.expr_decoder.fc.2.weight\n",
      "✅ TRAINABLE: base.module.expr_decoder.fc.2.bias\n",
      "✅ TRAINABLE: base.module.expr_decoder.fc.4.weight\n",
      "✅ TRAINABLE: base.module.expr_decoder.fc.4.bias\n",
      "✅ TRAINABLE: classifier.weight\n",
      "✅ TRAINABLE: classifier.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"✅ TRAINABLE: {name}\")\n",
    "    else:\n",
    "        print(f\"⛔️ FROZEN: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 2.9231, Accuracy: 0.0509\n",
      "Epoch 2 - Loss: 2.8840, Accuracy: 0.0229\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 23\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Accuracy calculation\u001b[39;00m\n\u001b[1;32m     26\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        values = batch[\"values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(input_ids=input_ids, attention_mask=attention_mask, values=values, labels=labels)\n",
    "        loss = out[\"loss\"]\n",
    "        logits = out[\"logits\"]\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Accuracy calculation\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "test_tokens = tokenizer.tokenize_cell_vectors(adata_test.X.toarray(), gene_names)\n",
    "test_dataset = scRNADataset(test_tokens)\n",
    "test_loader = DataLoader(test_dataset, batch_size=96, collate_fn=collate_fn)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        values = batch[\"values\"].to(device)  # ADD THIS LINE\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask, values=values)[\"logits\"]\n",
    "        batch_preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        preds.extend(batch_preds)\n",
    "\n",
    "# Map predictions to labels and save\n",
    "adata_test.obs[\"predicted_cell_type\"] = le.inverse_transform(preds)\n",
    "output_path = os.path.join(data_path, \"filtered_ms_adata_with_predictions.h5ad\")\n",
    "adata_test.write(output_path)\n",
    "print(f\"✅ Predictions saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_data = sc.read_h5ad(os.path.join(data_path, \"filtered_ms_adata_with_predictions.h5ad\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_data.obs.predicted_cell_type.unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
