{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal scGPT Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s5srinivasan/py39env/lib64/python3.9/site-packages/scgpt/model/model.py:21: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n",
      "/home/s5srinivasan/py39env/lib64/python3.9/site-packages/scgpt/model/multiomic_model.py:19: UserWarning: flash_attn is not installed\n",
      "  warnings.warn(\"flash_attn is not installed\")\n",
      "/home/s5srinivasan/py39env/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n"
     ]
    }
   ],
   "source": [
    "#############################################################\n",
    "import sys\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "#############################################################\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import Vocab as VocabPybind\n",
    "\n",
    "#############################################################\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "#############################################################\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "#############################################################\n",
    "import scgpt as scg\n",
    "from scgpt.model import TransformerModel\n",
    "from scgpt.tokenizer import tokenize_and_pad_batch, random_mask_value\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.preprocess import Preprocessor\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set device and seed\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for cell type classification\n",
    "seed = 0\n",
    "dataset_name = \"ms\"\n",
    "mask_ratio = 0.0\n",
    "epochs = 10\n",
    "n_bins = 51\n",
    "lr = 1e-4\n",
    "batch_size = 32\n",
    "eval_batch_size = 32\n",
    "layer_size = 128\n",
    "nlayers = 4\n",
    "nhead = 4\n",
    "dropout = 0.2\n",
    "schedule_ratio = 0.9\n",
    "fast_transformer = True\n",
    "pre_norm = False\n",
    "amp = True  # Automatic Mixed Precision\n",
    "include_zero_gene = False\n",
    "freeze = False  # Whether to freeze pre-trained layers\n",
    "\n",
    "# Settings for input and preprocessing\n",
    "pad_token = \"\"\n",
    "special_tokens = [pad_token, \"\", \"\"]\n",
    "max_seq_len = 3001\n",
    "input_style = \"binned\"\n",
    "input_emb_style = \"continuous\"\n",
    "cell_emb_style = \"cls\"\n",
    "\n",
    "# Set values based on input style\n",
    "if input_emb_style == \"category\":\n",
    "    mask_value = n_bins + 1\n",
    "    pad_value = n_bins\n",
    "    n_input_bins = n_bins + 2\n",
    "else:\n",
    "    mask_value = -1\n",
    "    pad_value = -2\n",
    "    n_input_bins = n_bins\n",
    "\n",
    "# Create save directory\n",
    "save_dir = Path(f\"./save/dev_{dataset_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"Load training and test data using scanpy\"\"\"\n",
    "    data_dir = Path(\"../data/sample_ms\")\n",
    "    adata = sc.read(data_dir / \"c_data.h5ad\")\n",
    "    adata_test = sc.read(data_dir / \"filtered_ms_adata.h5ad\")\n",
    "    \n",
    "    # Setup celltype information\n",
    "    adata.obs[\"celltype\"] = adata.obs[\"Factor Value[inferred cell type - authors labels]\"].astype(\"category\")\n",
    "    adata_test.obs[\"celltype\"] = adata_test.obs[\"Factor Value[inferred cell type - authors labels]\"].astype(\"category\")\n",
    "    \n",
    "    # Setup batch information\n",
    "    adata.obs[\"batch_id\"] = adata.obs[\"str_batch\"] = \"0\"\n",
    "    adata_test.obs[\"batch_id\"] = adata_test.obs[\"str_batch\"] = \"1\"\n",
    "    \n",
    "    # Setup gene names\n",
    "    adata.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "    adata_test.var.set_index(adata.var[\"gene_name\"], inplace=True)\n",
    "    \n",
    "    # Concatenate data\n",
    "    adata_test_raw = adata_test.copy()\n",
    "    adata = adata.concatenate(adata_test, batch_key=\"str_batch\")\n",
    "    \n",
    "    # Setup celltype and batch categories\n",
    "    batch_id_labels = adata.obs[\"str_batch\"].astype(\"category\").cat.codes.values\n",
    "    adata.obs[\"batch_id\"] = batch_id_labels\n",
    "    \n",
    "    celltype_id_labels = adata.obs[\"celltype\"].astype(\"category\").cat.codes.values\n",
    "    adata.obs[\"celltype_id\"] = celltype_id_labels\n",
    "    \n",
    "    celltypes = adata.obs[\"celltype\"].unique()\n",
    "    num_types = len(np.unique(celltype_id_labels))\n",
    "    id2type = dict(enumerate(adata.obs[\"celltype\"].astype(\"category\").cat.categories))\n",
    "    \n",
    "    adata.var[\"gene_name\"] = adata.var.index.tolist()\n",
    "    \n",
    "    return adata, adata_test_raw, num_types, id2type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(adata, data_is_raw=False, filter_gene_by_counts=False):\n",
    "    \"\"\"Preprocess the data using scGPT preprocessor\"\"\"\n",
    "    preprocessor = Preprocessor(\n",
    "        use_key=\"X\",\n",
    "        filter_gene_by_counts=filter_gene_by_counts,\n",
    "        filter_cell_by_counts=False,\n",
    "        normalize_total=1e4,\n",
    "        result_normed_key=\"X_normed\",\n",
    "        log1p=data_is_raw,\n",
    "        result_log1p_key=\"X_log1p\",\n",
    "        subset_hvg=False,\n",
    "        hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "        binning=n_bins,\n",
    "        result_binned_key=\"X_binned\",\n",
    "    )\n",
    "    \n",
    "    # Separate test data\n",
    "    adata_test = adata[adata.obs[\"str_batch\"] == \"1\"].copy()\n",
    "    adata_train = adata[adata.obs[\"str_batch\"] == \"0\"].copy()\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    preprocessor(adata_train, batch_key=None)\n",
    "    preprocessor(adata_test, batch_key=None)\n",
    "    \n",
    "    return adata_train, adata_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(adata, model_dir):\n",
    "    \"\"\"Load vocabulary from a pre-trained model\"\"\"\n",
    "    model_dir = Path(model_dir)\n",
    "    vocab_file = model_dir / \"vocab.json\"\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    \n",
    "    # Copy vocabulary to save directory\n",
    "    shutil.copy(vocab_file, save_dir / \"vocab.json\")\n",
    "    \n",
    "    # Add special tokens if not already in vocab\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "    \n",
    "    # Check how many genes in the data match the vocabulary\n",
    "    adata.var[\"id_in_vocab\"] = [\n",
    "        1 if gene in vocab else -1 for gene in adata.var[\"gene_name\"]\n",
    "    ]\n",
    "    \n",
    "    gene_ids_in_vocab = np.array(adata.var[\"id_in_vocab\"])\n",
    "    logger.info(\n",
    "        f\"Match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(vocab)}.\"\n",
    "    )\n",
    "    \n",
    "    # Filter genes by vocabulary\n",
    "    adata = adata[:, adata.var[\"id_in_vocab\"] >= 0]\n",
    "    \n",
    "    return vocab, adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneExpressionDataset(Dataset):\n",
    "    \"\"\"Simple dataset class for gene expression data\"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.data[\"gene_ids\"].shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.data.items()}\n",
    "\n",
    "def prepare_dataloader(data_pt, batch_size, shuffle=False):\n",
    "    \"\"\"Prepare dataloader for the model\"\"\"\n",
    "    dataset = GeneExpressionDataset(data_pt)\n",
    "    \n",
    "    # Use multiple workers if available\n",
    "    num_workers = min(os.cpu_count() or 1, batch_size // 2)\n",
    "    \n",
    "    data_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(vocab, num_types, model_dir):\n",
    "    \"\"\"Load and set up the model for cell type classification\"\"\"\n",
    "    ntokens = len(vocab)\n",
    "    \n",
    "    model = TransformerModel(\n",
    "        ntokens,\n",
    "        embsize=layer_size,\n",
    "        nhead=nhead,\n",
    "        d_hid=layer_size,\n",
    "        nlayers=nlayers,\n",
    "        nlayers_cls=3,\n",
    "        n_cls=num_types,\n",
    "        vocab=vocab,\n",
    "        dropout=dropout,\n",
    "        pad_token=pad_token,\n",
    "        pad_value=pad_value,\n",
    "        # Disable unnecessary features\n",
    "        do_mvc=False,\n",
    "        do_dab=False,\n",
    "        use_batch_labels=False,\n",
    "        num_batch_labels=1,\n",
    "        domain_spec_batchnorm=False,\n",
    "        # Keep necessary settings\n",
    "        input_emb_style=input_emb_style,\n",
    "        n_input_bins=n_input_bins,\n",
    "        cell_emb_style=cell_emb_style,\n",
    "        mvc_decoder_style=\"inner product\",\n",
    "        ecs_threshold=0.0,\n",
    "        explicit_zero_prob=explicit_zero_prob,\n",
    "        use_fast_transformer=fast_transformer,\n",
    "        fast_transformer_backend=\"flash\",\n",
    "        pre_norm=pre_norm,\n",
    "    )\n",
    "    \n",
    "    # Load pre-trained weights\n",
    "    model_file = model_dir / \"best_model.pt\"\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "        logger.info(f\"Loading all model params from {model_file}\")\n",
    "    except:\n",
    "        # Only load matching parameters\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = torch.load(model_file)\n",
    "        pretrained_dict = {\n",
    "            k: v for k, v in pretrained_dict.items()\n",
    "            if k in model_dict and v.shape == model_dict[k].shape\n",
    "        }\n",
    "        \n",
    "        for k, v in pretrained_dict.items():\n",
    "            logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "        \n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "    \n",
    "    # Freeze parameters if requested\n",
    "    if freeze:\n",
    "        for name, para in model.named_parameters():\n",
    "            if \"encoder\" in name and \"transformer_encoder\" not in name:\n",
    "                logger.info(f\"Freezing weights for: {name}\")\n",
    "                para.requires_grad = False\n",
    "    \n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, scaler, epoch):\n",
    "    \"\"\"Train the model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_error = 0.0\n",
    "    num_batches = len(loader)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch, batch_data in enumerate(loader):\n",
    "        input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "        input_values = batch_data[\"values\"].to(device)\n",
    "        celltype_labels = batch_data[\"celltype_labels\"].to(device)\n",
    "        \n",
    "        # Create padding mask\n",
    "        src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        with torch.cuda.amp.autocast(enabled=amp):\n",
    "            output_dict = model(\n",
    "                input_gene_ids,\n",
    "                input_values,\n",
    "                src_key_padding_mask=src_key_padding_mask,\n",
    "                batch_labels=None,\n",
    "                CLS=True,\n",
    "                CCE=False,\n",
    "                MVC=False,\n",
    "                ECS=False,\n",
    "                do_sample=False,\n",
    "            )\n",
    "            \n",
    "            # Only compute classification loss\n",
    "            loss = criterion(output_dict[\"cls_output\"], celltype_labels)\n",
    "        \n",
    "        # Error rate calculation\n",
    "        error_rate = 1 - ((output_dict[\"cls_output\"].argmax(1) == celltype_labels).sum().item()) / celltype_labels.size(0)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item()\n",
    "        total_error += error_rate\n",
    "        \n",
    "        # Log progress\n",
    "        if batch % 100 == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / 100\n",
    "            cur_loss = total_loss / 100\n",
    "            cur_error = total_error / 100\n",
    "            \n",
    "            logger.info(\n",
    "                f\"| epoch {epoch:3d} | {batch:3d}/{num_batches:3d} batches | \"\n",
    "                f\"lr {lr:05.4f} | ms/batch {ms_per_batch:5.2f} | \"\n",
    "                f\"loss {cur_loss:5.2f} | err {cur_error:5.2f}\"\n",
    "            )\n",
    "            \n",
    "            total_loss = 0\n",
    "            total_error = 0\n",
    "            start_time = time.time()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    \"\"\"Evaluate model performance on provided dataloader\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_error = 0.0\n",
    "    total_num = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_data in loader:\n",
    "            input_gene_ids = batch_data[\"gene_ids\"].to(device)\n",
    "            input_values = batch_data[\"values\"].to(device)\n",
    "            celltype_labels = batch_data[\"celltype_labels\"].to(device)\n",
    "            \n",
    "            src_key_padding_mask = input_gene_ids.eq(vocab[pad_token])\n",
    "            \n",
    "            with torch.cuda.amp.autocast(enabled=amp):\n",
    "                output_dict = model(\n",
    "                    input_gene_ids,\n",
    "                    input_values,\n",
    "                    src_key_padding_mask=src_key_padding_mask,\n",
    "                    batch_labels=None,\n",
    "                    CLS=True,\n",
    "                    CCE=False,\n",
    "                    MVC=False,\n",
    "                    ECS=False,\n",
    "                    do_sample=False,\n",
    "                )\n",
    "                \n",
    "                output_values = output_dict[\"cls_output\"]\n",
    "                loss = criterion(output_values, celltype_labels)\n",
    "            \n",
    "            total_loss += loss.item() * len(input_gene_ids)\n",
    "            accuracy = (output_values.argmax(1) == celltype_labels).sum().item()\n",
    "            total_error += (1 - accuracy / len(input_gene_ids)) * len(input_gene_ids)\n",
    "            total_num += len(input_gene_ids)\n",
    "            \n",
    "            preds = output_values.argmax(1).cpu().numpy()\n",
    "            predictions.append(preds)\n",
    "    \n",
    "    return total_loss / total_num, total_error / total_num, np.concatenate(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Set seed for reproducibility\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Load data\n",
    "    logger.info(\"Loading data...\")\n",
    "    adata, adata_test_raw, num_types, id2type = load_data()\n",
    "    \n",
    "    # Load vocabulary\n",
    "    logger.info(\"Loading vocabulary...\")\n",
    "    vocab, adata = load_vocab(adata, model_dir=\"../save/scGPT_human\")\n",
    "    \n",
    "    # Preprocess data\n",
    "    logger.info(\"Preprocessing data...\")\n",
    "    adata_train, adata_test = preprocess_data(adata)\n",
    "    \n",
    "    # Determine input layer key based on input style\n",
    "    input_layer_key = {\n",
    "        \"normed_raw\": \"X_normed\",\n",
    "        \"log1p\": \"X_normed\",\n",
    "        \"binned\": \"X_binned\",\n",
    "    }[input_style]\n",
    "    \n",
    "    # Prepare tokenized training data\n",
    "    train_data_pt, valid_data_pt, gene_ids = prepare_training_data(adata_train, vocab, input_layer_key)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = prepare_dataloader(train_data_pt, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = prepare_dataloader(valid_data_pt, batch_size=eval_batch_size)\n",
    "    \n",
    "    # Setup model\n",
    "    model = setup_model(vocab, num_types, model_dir=\"../save/scGPT_human\")\n",
    "    \n",
    "    # Setup optimizer and criterion\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=schedule_ratio)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Train one epoch\n",
    "        model = train_epoch(model, train_loader, optimizer, criterion, scaler, epoch)\n",
    "        \n",
    "        # Evaluate\n",
    "        val_loss, val_err, _ = evaluate(model, valid_loader)\n",
    "        \n",
    "        # Log results\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        logger.info(\"-\" * 89)\n",
    "        logger.info(\n",
    "            f\"| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | \"\n",
    "            f\"valid loss {val_loss:5.4f} | err {val_err:5.4f}\"\n",
    "        )\n",
    "        logger.info(\"-\" * 89)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_model_epoch = epoch\n",
    "            logger.info(f\"Best model with score {best_val_loss:5.4f}\")\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    # Inference on test data\n",
    "    predictions, labels, results = inference(best_model, adata_test, vocab, gene_ids, input_layer_key)\n",
    "    \n",
    "    # Save predictions to test data h5ad object\n",
    "    adata_test_raw.obs[\"predictions\"] = [id2type[p] for p in predictions]\n",
    "    adata_test_raw.write(save_dir / \"test_with_predictions.h5ad\")\n",
    "    \n",
    "    # Save model and results\n",
    "    torch.save(best_model.state_dict(), save_dir / \"model.pt\")\n",
    "    \n",
    "    with open(save_dir / \"results.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            \"predictions\": predictions,\n",
    "            \"labels\": labels,\n",
    "            \"results\": results,\n",
    "            \"id_maps\": id2type\n",
    "        }, f)\n",
    "    \n",
    "    logger.info(f\"Final metrics: {results}\")\n",
    "    logger.info(\"Pipeline completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 14:10:43,091 - __main__ - INFO - Loading data...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = '../data/ms/c_data.h5ad', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m      6\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m adata, adata_test_raw, num_types, id2type \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load vocabulary\u001b[39;00m\n\u001b[1;32m     10\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading vocabulary...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load training and test data using scanpy\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m adata \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mc_data.h5ad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m adata_test \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mread(data_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiltered_ms_adata.h5ad\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Setup celltype information\u001b[39;00m\n",
      "File \u001b[0;32m~/py39env/lib64/python3.9/site-packages/legacy_api_wrap/__init__.py:82\u001b[0m, in \u001b[0;36mlegacy_api.<locals>.wrapper.<locals>.fn_compatible\u001b[0;34m(*args_all, **kw)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn_compatible\u001b[39m(\u001b[38;5;241m*\u001b[39margs_all: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m R:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args_all) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n_positional:\n\u001b[0;32m---> 82\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     args_pos: P\u001b[38;5;241m.\u001b[39margs\n\u001b[1;32m     85\u001b[0m     args_pos, args_rest \u001b[38;5;241m=\u001b[39m args_all[:n_positional], args_all[n_positional:]\n",
      "File \u001b[0;32m~/py39env/lib64/python3.9/site-packages/scanpy/readwrite.py:130\u001b[0m, in \u001b[0;36mread\u001b[0;34m(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m filename \u001b[38;5;241m=\u001b[39m Path(filename)  \u001b[38;5;66;03m# allow passing strings\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_valid_filename(filename):\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbacked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbacked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43msheet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfirst_column_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_column_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackup_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackup_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_compression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# generate filename and read to dict\u001b[39;00m\n\u001b[1;32m    143\u001b[0m filekey \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(filename)\n",
      "File \u001b[0;32m~/py39env/lib64/python3.9/site-packages/scanpy/readwrite.py:766\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filename, backed, sheet, ext, delimiter, first_column_names, backup_url, cache, cache_compression, suppress_cache_warning, **kwargs)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh5ad\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sheet \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 766\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread_h5ad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbacked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbacked\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    768\u001b[0m         logg\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreading sheet \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msheet\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/py39env/lib64/python3.9/site-packages/anndata/_io/h5ad.py:234\u001b[0m, in \u001b[0;36mread_h5ad\u001b[0;34m(filename, backed, as_sparse, as_sparse_fmt, chunk_size)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    227\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently only `X` and `raw/X` can be read as sparse.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    228\u001b[0m         )\n\u001b[1;32m    230\u001b[0m rdasp \u001b[38;5;241m=\u001b[39m partial(\n\u001b[1;32m    231\u001b[0m     read_dense_as_sparse, sparse_format\u001b[38;5;241m=\u001b[39mas_sparse_fmt, axis_chunk\u001b[38;5;241m=\u001b[39mchunk_size\n\u001b[1;32m    232\u001b[0m )\n\u001b[0;32m--> 234\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcallback\u001b[39m(func, elem_name: \u001b[38;5;28mstr\u001b[39m, elem, iospec):\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m iospec\u001b[38;5;241m.\u001b[39mencoding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manndata\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m elem_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/py39env/lib64/python3.9/site-packages/h5py/_hl/files.py:561\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    552\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    553\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    554\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    555\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    556\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    557\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    558\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    559\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    560\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 561\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m~/py39env/lib64/python3.9/site-packages/h5py/_hl/files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = '../data/ms/c_data.h5ad', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
